{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading data...\n",
      "Found 121 video files\n",
      "Found 121 transcription files\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_001.txt\n",
      "Added transcription for trial_lie_001\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_002.txt\n",
      "Added transcription for trial_lie_002\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_003.txt\n",
      "Added transcription for trial_lie_003\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_004.txt\n",
      "Added transcription for trial_lie_004\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_005.txt\n",
      "Added transcription for trial_lie_005\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_006.txt\n",
      "Added transcription for trial_lie_006\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_007.txt\n",
      "Added transcription for trial_lie_007\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_008.txt\n",
      "Added transcription for trial_lie_008\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_009.txt\n",
      "Added transcription for trial_lie_009\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_010.txt\n",
      "Added transcription for trial_lie_010\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_011.txt\n",
      "Added transcription for trial_lie_011\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_012.txt\n",
      "Added transcription for trial_lie_012\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_013.txt\n",
      "Added transcription for trial_lie_013\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_014.txt\n",
      "Added transcription for trial_lie_014\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_015.txt\n",
      "Added transcription for trial_lie_015\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_016.txt\n",
      "Added transcription for trial_lie_016\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_017.txt\n",
      "Added transcription for trial_lie_017\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_018.txt\n",
      "Added transcription for trial_lie_018\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_019.txt\n",
      "Added transcription for trial_lie_019\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_020.txt\n",
      "Added transcription for trial_lie_020\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_021.txt\n",
      "Added transcription for trial_lie_021\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_022.txt\n",
      "Added transcription for trial_lie_022\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_023.txt\n",
      "Added transcription for trial_lie_023\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_024.txt\n",
      "Added transcription for trial_lie_024\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_025.txt\n",
      "Added transcription for trial_lie_025\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_026.txt\n",
      "Added transcription for trial_lie_026\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_027.txt\n",
      "Added transcription for trial_lie_027\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_028.txt\n",
      "Added transcription for trial_lie_028\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_029.txt\n",
      "Added transcription for trial_lie_029\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_030.txt\n",
      "Added transcription for trial_lie_030\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_031.txt\n",
      "Added transcription for trial_lie_031\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_032.txt\n",
      "Added transcription for trial_lie_032\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_033.txt\n",
      "Added transcription for trial_lie_033\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_034.txt\n",
      "Added transcription for trial_lie_034\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_035.txt\n",
      "Added transcription for trial_lie_035\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_036.txt\n",
      "Added transcription for trial_lie_036\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_037.txt\n",
      "Added transcription for trial_lie_037\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_038.txt\n",
      "Added transcription for trial_lie_038\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_039.txt\n",
      "Added transcription for trial_lie_039\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_040.txt\n",
      "Added transcription for trial_lie_040\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_041.txt\n",
      "Added transcription for trial_lie_041\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_042.txt\n",
      "Added transcription for trial_lie_042\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_043.txt\n",
      "Added transcription for trial_lie_043\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_044.txt\n",
      "Added transcription for trial_lie_044\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_045.txt\n",
      "Added transcription for trial_lie_045\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_046.txt\n",
      "Added transcription for trial_lie_046\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_047.txt\n",
      "Added transcription for trial_lie_047\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_048.txt\n",
      "Added transcription for trial_lie_048\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_049.txt\n",
      "Added transcription for trial_lie_049\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_050.txt\n",
      "Added transcription for trial_lie_050\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_051.txt\n",
      "Added transcription for trial_lie_051\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_052.txt\n",
      "Added transcription for trial_lie_052\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_053.txt\n",
      "Added transcription for trial_lie_053\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_054.txt\n",
      "Added transcription for trial_lie_054\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_055.txt\n",
      "Added transcription for trial_lie_055\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_056.txt\n",
      "Added transcription for trial_lie_056\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_057.txt\n",
      "Added transcription for trial_lie_057\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_058.txt\n",
      "Added transcription for trial_lie_058\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_059.txt\n",
      "Added transcription for trial_lie_059\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_060.txt\n",
      "Added transcription for trial_lie_060\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Deceptive\\trial_lie_061.txt\n",
      "Added transcription for trial_lie_061\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_001.txt\n",
      "Added transcription for trial_truth_001\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_002.txt\n",
      "Added transcription for trial_truth_002\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_003.txt\n",
      "Added transcription for trial_truth_003\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_004.txt\n",
      "Added transcription for trial_truth_004\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_005.txt\n",
      "Added transcription for trial_truth_005\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_006.txt\n",
      "Added transcription for trial_truth_006\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_007.txt\n",
      "Added transcription for trial_truth_007\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_008.txt\n",
      "Added transcription for trial_truth_008\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_009.txt\n",
      "Added transcription for trial_truth_009\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_010.txt\n",
      "Added transcription for trial_truth_010\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_011.txt\n",
      "Added transcription for trial_truth_011\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_012.txt\n",
      "Added transcription for trial_truth_012\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_013.txt\n",
      "Added transcription for trial_truth_013\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_014.txt\n",
      "Added transcription for trial_truth_014\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_015.txt\n",
      "Added transcription for trial_truth_015\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_016.txt\n",
      "Added transcription for trial_truth_016\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_017.txt\n",
      "Added transcription for trial_truth_017\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_018.txt\n",
      "Added transcription for trial_truth_018\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_019.txt\n",
      "Added transcription for trial_truth_019\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_020.txt\n",
      "Added transcription for trial_truth_020\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_021.txt\n",
      "Added transcription for trial_truth_021\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_022.txt\n",
      "Added transcription for trial_truth_022\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_023.txt\n",
      "Added transcription for trial_truth_023\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_024.txt\n",
      "Added transcription for trial_truth_024\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_025.txt\n",
      "Added transcription for trial_truth_025\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_026.txt\n",
      "Added transcription for trial_truth_026\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_027.txt\n",
      "Added transcription for trial_truth_027\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_028.txt\n",
      "Added transcription for trial_truth_028\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_029.txt\n",
      "Added transcription for trial_truth_029\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_030.txt\n",
      "Added transcription for trial_truth_030\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_031.txt\n",
      "Added transcription for trial_truth_031\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_032.txt\n",
      "Added transcription for trial_truth_032\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_033.txt\n",
      "Added transcription for trial_truth_033\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_034.txt\n",
      "Added transcription for trial_truth_034\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_035.txt\n",
      "Added transcription for trial_truth_035\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_036.txt\n",
      "Added transcription for trial_truth_036\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_037.txt\n",
      "Added transcription for trial_truth_037\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_038.txt\n",
      "Added transcription for trial_truth_038\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_039.txt\n",
      "Added transcription for trial_truth_039\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_040.txt\n",
      "Added transcription for trial_truth_040\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_041.txt\n",
      "Added transcription for trial_truth_041\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_042.txt\n",
      "Added transcription for trial_truth_042\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_043.txt\n",
      "Added transcription for trial_truth_043\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_044.txt\n",
      "Added transcription for trial_truth_044\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_045.txt\n",
      "Added transcription for trial_truth_045\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_046.txt\n",
      "Added transcription for trial_truth_046\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_047.txt\n",
      "Added transcription for trial_truth_047\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_048.txt\n",
      "Added transcription for trial_truth_048\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_049.txt\n",
      "Added transcription for trial_truth_049\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_050.txt\n",
      "Added transcription for trial_truth_050\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_051.txt\n",
      "Added transcription for trial_truth_051\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_052.txt\n",
      "Added transcription for trial_truth_052\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_053.txt\n",
      "Added transcription for trial_truth_053\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_054.txt\n",
      "Added transcription for trial_truth_054\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_055.txt\n",
      "Added transcription for trial_truth_055\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_056.txt\n",
      "Added transcription for trial_truth_056\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_057.txt\n",
      "Added transcription for trial_truth_057\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_058.txt\n",
      "Added transcription for trial_truth_058\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_059.txt\n",
      "Added transcription for trial_truth_059\n",
      "Reading transcription file: Real-life_Deception_Detection_2016\\Transcription\\Truthful\\trial_truth_060.txt\n",
      "Added transcription for trial_truth_060\n",
      "Data loading complete. Found 121 synchronized trials.\n",
      "Starting mandatory facial recognition for subject identification...\n",
      "Facial recognition complete. Identified 44 unique subjects and 12 videos needing unique IDs.\n",
      "Preparing data for LOSO...\n",
      "Data preparation complete. 121 trials ready for LOSO.\n",
      "Extracting NLP features (BERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP feature extraction complete.\n",
      "Extracting Audio features (MFCCs)...\n",
      "Audio feature extraction complete.\n",
      "Extracting Visual features (ResNet)...\n",
      "Visual feature extraction complete.\n",
      "Feature shapes: NLP=(121, 768), Audio=(121, 26), Visual=(121, 512)\n",
      "Starting LOSO Cross-Validation...\n",
      "\n",
      "--- Fold 1/56: Testing on Subject subject_1 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 1 Average (across seeds) - Accuracy: 0.4722, F1: 0.4804\n",
      "\n",
      "--- Fold 2/56: Testing on Subject subject_10 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 2 Average (across seeds) - Accuracy: 0.4444, F1: 0.2889\n",
      "\n",
      "--- Fold 3/56: Testing on Subject subject_11 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 3 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 4/56: Testing on Subject subject_12 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 4 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 5/56: Testing on Subject subject_13 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 5 Average (across seeds) - Accuracy: 0.9167, F1: 0.9524\n",
      "\n",
      "--- Fold 6/56: Testing on Subject subject_14 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 6 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 7/56: Testing on Subject subject_15 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 7 Average (across seeds) - Accuracy: 0.7778, F1: 0.7222\n",
      "\n",
      "--- Fold 8/56: Testing on Subject subject_16 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 8 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 9/56: Testing on Subject subject_17 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 9 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 10/56: Testing on Subject subject_18 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 10 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 11/56: Testing on Subject subject_19 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 11 Average (across seeds) - Accuracy: 0.6667, F1: 0.5556\n",
      "\n",
      "--- Fold 12/56: Testing on Subject subject_2 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 12 Average (across seeds) - Accuracy: 0.6333, F1: 0.5343\n",
      "\n",
      "--- Fold 13/56: Testing on Subject subject_20 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 13 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 14/56: Testing on Subject subject_21 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 14 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 15/56: Testing on Subject subject_22 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 15 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 16/56: Testing on Subject subject_23 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 16 Average (across seeds) - Accuracy: 0.3333, F1: 0.4444\n",
      "\n",
      "--- Fold 17/56: Testing on Subject subject_24 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 17 Average (across seeds) - Accuracy: 0.5833, F1: 0.6190\n",
      "\n",
      "--- Fold 18/56: Testing on Subject subject_25 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 18 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 19/56: Testing on Subject subject_26 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 19 Average (across seeds) - Accuracy: 0.0000, F1: 0.0000\n",
      "\n",
      "--- Fold 20/56: Testing on Subject subject_27 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 20 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 21/56: Testing on Subject subject_28 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 21 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 22/56: Testing on Subject subject_29 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 22 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 23/56: Testing on Subject subject_3 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 23 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 24/56: Testing on Subject subject_30 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 24 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 25/56: Testing on Subject subject_31 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 25 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 26/56: Testing on Subject subject_32 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 26 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 27/56: Testing on Subject subject_33 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 27 Average (across seeds) - Accuracy: 0.8333, F1: 0.8889\n",
      "\n",
      "--- Fold 28/56: Testing on Subject subject_34 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 28 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 29/56: Testing on Subject subject_35 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 29 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 30/56: Testing on Subject subject_36 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 30 Average (across seeds) - Accuracy: 0.0000, F1: 0.0000\n",
      "\n",
      "--- Fold 31/56: Testing on Subject subject_37 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 31 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 32/56: Testing on Subject subject_38 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 32 Average (across seeds) - Accuracy: 0.0000, F1: 0.0000\n",
      "\n",
      "--- Fold 33/56: Testing on Subject subject_39 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 33 Average (across seeds) - Accuracy: 0.1667, F1: 0.2222\n",
      "\n",
      "--- Fold 34/56: Testing on Subject subject_4 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 34 Average (across seeds) - Accuracy: 0.4444, F1: 0.4091\n",
      "\n",
      "--- Fold 35/56: Testing on Subject subject_40 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 35 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 36/56: Testing on Subject subject_41 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 36 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 37/56: Testing on Subject subject_42 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 37 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 38/56: Testing on Subject subject_43 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 38 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 39/56: Testing on Subject subject_44 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 39 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 40/56: Testing on Subject subject_5 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 40 Average (across seeds) - Accuracy: 0.8889, F1: 0.9333\n",
      "\n",
      "--- Fold 41/56: Testing on Subject subject_6 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 41 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 42/56: Testing on Subject subject_7 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 42 Average (across seeds) - Accuracy: 0.0000, F1: 0.0000\n",
      "\n",
      "--- Fold 43/56: Testing on Subject subject_8 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 43 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 44/56: Testing on Subject subject_9 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 44 Average (across seeds) - Accuracy: 0.5000, F1: 0.3333\n",
      "\n",
      "--- Fold 45/56: Testing on Subject unknown_no_face_1 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 45 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 46/56: Testing on Subject unknown_no_face_10 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 46 Average (across seeds) - Accuracy: 0.0000, F1: 0.0000\n",
      "\n",
      "--- Fold 47/56: Testing on Subject unknown_no_face_11 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 47 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 48/56: Testing on Subject unknown_no_face_12 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 48 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 49/56: Testing on Subject unknown_no_face_2 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 49 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 50/56: Testing on Subject unknown_no_face_3 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 50 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333\n",
      "\n",
      "--- Fold 51/56: Testing on Subject unknown_no_face_4 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 51 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 52/56: Testing on Subject unknown_no_face_5 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 52 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 53/56: Testing on Subject unknown_no_face_6 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 53 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 54/56: Testing on Subject unknown_no_face_7 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 54 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667\n",
      "\n",
      "--- Fold 55/56: Testing on Subject unknown_no_face_8 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 55 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Fold 56/56: Testing on Subject unknown_no_face_9 ---\n",
      "  Seed 1/3\n",
      "  Seed 2/3\n",
      "  Seed 3/3\n",
      "  Fold 56 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000\n",
      "\n",
      "--- Overall LOSO Results ---\n",
      "Overall Average Accuracy: 0.6249\n",
      "Overall Average F1-score: 0.6200\n",
      "\n",
      "Total execution time: 17.59 minutes\n",
      "Multimodal training complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import re\n",
    "import cv2 # OpenCV for video processing\n",
    "import face_recognition # Now mandatory\n",
    "import librosa # For audio analysis\n",
    "from moviepy.editor import VideoFileClip # For extracting audio\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from l\n",
    "# ibraries like moviepy/librosa if needed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "VISUAL_MODEL_NAME = 'resnet18' # Using ResNet18 for visual features\n",
    "AUDIO_N_MFCC = 13 # Number of MFCCs for audio features\n",
    "VISUAL_FRAMES_TO_SAMPLE = 70 # Number of frames to sample per video for visual features\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Data Directory Setup ---\n",
    "BASE_DIR = os.getcwd()  # Get current working directory\n",
    "data_dir = os.path.join(BASE_DIR, 'Real-life_Deception_Detection_2016')\n",
    "annotation_file = os.path.join(data_dir, 'Annotation', 'All_Gestures_Deceptive and Truthful.csv')\n",
    "# --- 1. Subject Identification (Mandatory Facial Recognition) ---\n",
    "def identify_subjects_facial_recognition(data):\n",
    "    \"\"\"\n",
    "    Identifies subjects MANDATORILY using facial recognition from the first frame.\n",
    "    Maps trial_id to a subject label.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries from load_data, containing 'video_path' and 'video_id' (trial_id).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping trial_ids (video_ids) to subject labels (e.g., 'subject_1', 'unknown_trial_xyz').\n",
    "    \"\"\"\n",
    "    print(\"Starting mandatory facial recognition for subject identification...\")\n",
    "    subject_mapping = {}\n",
    "    known_faces = {}  # Store known face encodings and labels {label: encoding}\n",
    "    subject_counter = 1\n",
    "    unknown_counter = 1\n",
    "\n",
    "    for item in data:\n",
    "        video_path = item['video_path']\n",
    "        trial_id = item['video_id'] # Use the actual trial_id\n",
    "        subject_label = None\n",
    "\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Warning: Could not open video file {video_path} for trial {trial_id}. Assigning unknown subject.\")\n",
    "                subject_label = f'unknown_video_open_error_{unknown_counter}'\n",
    "                unknown_counter += 1\n",
    "                subject_mapping[trial_id] = subject_label\n",
    "                continue\n",
    "\n",
    "            # Read the first frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Warning: Could not read frame from video {video_path} for trial {trial_id}. Assigning unknown subject.\")\n",
    "                subject_label = f'unknown_frame_read_error_{unknown_counter}'\n",
    "                unknown_counter += 1\n",
    "                subject_mapping[trial_id] = subject_label\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            # Convert the frame to RGB (face_recognition uses RGB)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Find face locations and encodings in the frame\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            if not face_encodings:\n",
    "                # print(f\"Warning: No faces found in the first frame of video {trial_id}. Assigning unique unknown subject.\")\n",
    "                subject_label = f'unknown_no_face_{unknown_counter}' # Assign a unique unknown label based on trial id\n",
    "                unknown_counter += 1\n",
    "            else:\n",
    "                # Use the first face found\n",
    "                current_face_encoding = face_encodings[0]\n",
    "\n",
    "                # Check for matches with known faces\n",
    "                match_found = False\n",
    "                known_labels = list(known_faces.keys())\n",
    "                if known_labels:\n",
    "                    known_encodings = list(known_faces.values())\n",
    "                    # Increase tolerance slightly if needed, default is 0.6\n",
    "                    matches = face_recognition.compare_faces(known_encodings, current_face_encoding, tolerance=0.6)\n",
    "                    # Find the first match\n",
    "                    try:\n",
    "                        first_match_index = matches.index(True)\n",
    "                        subject_label = known_labels[first_match_index]\n",
    "                        match_found = True\n",
    "                    except ValueError: # No True value found in matches\n",
    "                        pass\n",
    "\n",
    "                if not match_found:\n",
    "                    # If no match, add the face to known faces\n",
    "                    subject_label = f'subject_{subject_counter}'\n",
    "                    known_faces[subject_label] = current_face_encoding\n",
    "                    subject_counter += 1\n",
    "\n",
    "            subject_mapping[trial_id] = subject_label\n",
    "            cap.release()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path} for trial {trial_id}: {e}. Assigning unknown subject.\")\n",
    "            subject_label = f'unknown_processing_error_{unknown_counter}'\n",
    "            unknown_counter +=1\n",
    "            subject_mapping[trial_id] = subject_label\n",
    "            if 'cap' in locals() and cap.isOpened():\n",
    "                cap.release()\n",
    "\n",
    "    print(f\"Facial recognition complete. Identified {subject_counter - 1} unique subjects and {unknown_counter - 1} videos needing unique IDs.\")\n",
    "    return subject_mapping\n",
    "\n",
    "\n",
    "# --- 2. Data Loading (Slightly modified for clarity) ---\n",
    "def load_data(data_dir, annotation_file):\n",
    "    \"\"\"Loads and synchronizes annotation, transcription, and video data.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Construct paths\n",
    "    clip_dirs = [\n",
    "        os.path.join(data_dir, 'Clips', 'Deceptive'),\n",
    "        os.path.join(data_dir, 'Clips', 'Truthful')\n",
    "    ]\n",
    "    transcript_dirs = [\n",
    "        os.path.join(data_dir, 'Transcription', 'Deceptive'),\n",
    "        os.path.join(data_dir, 'Transcription', 'Truthful')\n",
    "    ]\n",
    "    \n",
    "    # Initialize lists to store paths\n",
    "    video_paths = []\n",
    "    transcription_paths = []\n",
    "    \n",
    "    # Load video paths\n",
    "    for clip_dir in clip_dirs:\n",
    "        if os.path.isdir(clip_dir):\n",
    "            for filename in os.listdir(clip_dir):\n",
    "                if filename.endswith(\".mp4\"):\n",
    "                    video_paths.append(os.path.join(clip_dir, filename))\n",
    "        else:\n",
    "            print(f\"Warning: Clip directory not found: {clip_dir}\")\n",
    "    \n",
    "    # Load transcription paths - UPDATED to look for .txt files\n",
    "    for transcript_dir in transcript_dirs:\n",
    "        if os.path.isdir(transcript_dir):\n",
    "            for filename in os.listdir(transcript_dir):\n",
    "                if filename.endswith(\".txt\"):  # Changed from .csv to .txt\n",
    "                    transcription_paths.append(os.path.join(transcript_dir, filename))\n",
    "        else:\n",
    "            print(f\"Warning: Transcription directory not found: {transcript_dir}\")\n",
    "    \n",
    "    # Load annotations\n",
    "    try:\n",
    "        annotations_df = pd.read_csv(annotation_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Annotation file not found at {annotation_file}\")\n",
    "        return []\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    video_path_dict = {}\n",
    "    for video_path in video_paths:\n",
    "        video_filename = os.path.basename(video_path)\n",
    "        match = re.search(r\"trial_(truth|lie)_(\\d+)\\.mp4\", video_filename)\n",
    "        if match:\n",
    "            trial_id = f\"trial_{match.group(1)}_{match.group(2)}\"\n",
    "            video_path_dict[trial_id] = video_path\n",
    "    \n",
    "    # Print debug information\n",
    "    print(f\"Found {len(video_paths)} video files\")\n",
    "    print(f\"Found {len(transcription_paths)} transcription files\")\n",
    "    \n",
    "    transcription_dict = {}\n",
    "    for transcript_path in transcription_paths:\n",
    "        try:\n",
    "            # Read the .txt file directly\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                transcription_text = f.read().strip()\n",
    "            \n",
    "            # Extract trial_id from filename\n",
    "            filename = os.path.basename(transcript_path)\n",
    "            trial_id = filename.replace('.txt', '')  # Remove .txt extension\n",
    "            \n",
    "            print(f\"Reading transcription file: {transcript_path}\")\n",
    "            transcription_dict[trial_id] = transcription_text\n",
    "            print(f\"Added transcription for {trial_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading transcription file {transcript_path}: {e}\")\n",
    "    \n",
    "    # Synchronize data\n",
    "    synchronized_data = []\n",
    "    processed_ids = set()\n",
    "    \n",
    "    if 'id' not in annotations_df.columns or 'class' not in annotations_df.columns:\n",
    "        print(f\"Error: Annotation file missing 'id' or 'class' column.\")\n",
    "        return []\n",
    "    \n",
    "    for _, row in annotations_df.iterrows():\n",
    "        trial_id = str(row['id']).replace('.mp4', '')\n",
    "        annotation_label = row['class']\n",
    "        \n",
    "        if annotation_label not in ['truthful', 'deceptive']:\n",
    "            print(f\"Warning: Skipping trial {trial_id} due to unexpected class label: {annotation_label}\")\n",
    "            continue\n",
    "        \n",
    "        if trial_id in processed_ids:\n",
    "            print(f\"Warning: Duplicate trial ID {trial_id} found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        transcription_text = transcription_dict.get(trial_id)\n",
    "        video_path = video_path_dict.get(trial_id)\n",
    "        \n",
    "        if transcription_text is None:\n",
    "            print(f\"Warning: Transcription not found for trial {trial_id}\")\n",
    "            continue\n",
    "        if video_path is None:\n",
    "            print(f\"Warning: Video path not found for trial {trial_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Map labels to numerical values\n",
    "        label_map = {'truthful': 0, 'deceptive': 1}\n",
    "        numeric_label = label_map.get(annotation_label)\n",
    "        if numeric_label is None:\n",
    "            print(f\"Warning: Could not map label '{annotation_label}' for trial {trial_id}\")\n",
    "            continue\n",
    "        \n",
    "        synchronized_data.append({\n",
    "            'annotation': numeric_label,\n",
    "            'transcription': transcription_text,\n",
    "            'video_id': trial_id,\n",
    "            'video_path': video_path\n",
    "        })\n",
    "        processed_ids.add(trial_id)\n",
    "    \n",
    "    print(f\"Data loading complete. Found {len(synchronized_data)} synchronized trials.\")\n",
    "    return synchronized_data\n",
    "\n",
    "\n",
    "# --- 3. Feature Extraction ---\n",
    "\n",
    "# 3.1 NLP Feature Extraction (BERT) - Unchanged conceptually\n",
    "def extract_nlp_features(transcriptions):\n",
    "    \"\"\" Extracts BERT embeddings for a list of transcriptions. \"\"\"\n",
    "    print(\"Extracting NLP features (BERT)...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "    model = BertModel.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "    nlp_features = []\n",
    "    with torch.no_grad():\n",
    "        for i, text in enumerate(transcriptions):\n",
    "            try:\n",
    "                # Ensure text is a string\n",
    "                text = str(text) if text is not None else \"\"\n",
    "                if not text.strip(): # Handle empty strings\n",
    "                     print(f\"Warning: Empty transcription for item {i}. Using zero vector.\")\n",
    "                     # Get expected hidden size from model config\n",
    "                     hidden_size = model.config.hidden_size\n",
    "                     sentence_embedding = np.zeros((1, hidden_size))\n",
    "                else:\n",
    "                    inputs = tokenizer(text, return_tensors='pt',\n",
    "                                    truncation=True, padding=True, max_length=512).to(DEVICE) # Added max_length\n",
    "                    outputs = model(**inputs)\n",
    "                    # Mean of the last hidden state\n",
    "                    sentence_embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy() # (1, hidden_size)\n",
    "                nlp_features.append(sentence_embedding)\n",
    "            except Exception as e:\n",
    "                 print(f\"Error extracting NLP features for item {i}: {e}. Using zero vector.\")\n",
    "                 hidden_size = model.config.hidden_size\n",
    "                 nlp_features.append(np.zeros((1, hidden_size)))\n",
    "\n",
    "\n",
    "    print(\"NLP feature extraction complete.\")\n",
    "    # Ensure all features are arrays and handle potential shape issues before stacking\n",
    "    processed_features = []\n",
    "    target_shape = None\n",
    "    for feat in nlp_features:\n",
    "        if isinstance(feat, np.ndarray):\n",
    "             if target_shape is None:\n",
    "                 target_shape = feat.shape\n",
    "             # If shape mismatch, pad or truncate (or use zeros as done in exception handling)\n",
    "             if feat.shape != target_shape:\n",
    "                 print(f\"Warning: NLP feature shape mismatch {feat.shape} vs {target_shape}. Using zero vector.\")\n",
    "                 processed_features.append(np.zeros(target_shape))\n",
    "             else:\n",
    "                processed_features.append(feat)\n",
    "        else: # Should not happen if exceptions are caught, but as safeguard\n",
    "             if target_shape is None: # Need a shape defined first\n",
    "                 raise ValueError(\"Cannot process non-array NLP feature without a target shape.\")\n",
    "             print(f\"Warning: Non-array NLP feature found. Using zero vector.\")\n",
    "             processed_features.append(np.zeros(target_shape))\n",
    "\n",
    "    if not processed_features:\n",
    "        return np.array([]) # Return empty array if no features were processed\n",
    "\n",
    "    return np.vstack(processed_features) # (num_trials, hidden_size)\n",
    "\n",
    "\n",
    "# 3.2 Audio Feature Extraction (MFCCs)\n",
    "def extract_audio_features(video_paths):\n",
    "    \"\"\" Extracts MFCC features from the audio track of video files. \"\"\"\n",
    "    print(\"Extracting Audio features (MFCCs)...\")\n",
    "    audio_features = []\n",
    "    temp_audio_dir = \"temp_audio\"\n",
    "    if not os.path.exists(temp_audio_dir):\n",
    "        os.makedirs(temp_audio_dir)\n",
    "\n",
    "    num_features = AUDIO_N_MFCC * 2 # Mean and Std Dev for each MFCC\n",
    "\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        start_time = time.time()\n",
    "        temp_audio_path = os.path.join(temp_audio_dir, f\"temp_{i}.wav\")\n",
    "        feature_vector = np.zeros(num_features) # Default to zeros\n",
    "\n",
    "        try:\n",
    "            # Extract audio using moviepy\n",
    "            with VideoFileClip(video_path) as video_clip:\n",
    "                 if video_clip.audio is None:\n",
    "                      print(f\"Warning: Video {i} ({os.path.basename(video_path)}) has no audio track. Using zeros.\")\n",
    "                 else:\n",
    "                    video_clip.audio.write_audiofile(temp_audio_path, codec='pcm_s16le', logger=None) # Use logger=None to reduce console output\n",
    "\n",
    "            # Load audio and extract MFCCs using librosa\n",
    "            y, sr = librosa.load(temp_audio_path, sr=None) # Load with native sample rate\n",
    "            if len(y) > 0: # Check if audio signal is not empty\n",
    "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=AUDIO_N_MFCC)\n",
    "                mfccs_mean = np.mean(mfccs, axis=1)\n",
    "                mfccs_std = np.std(mfccs, axis=1)\n",
    "                feature_vector = np.concatenate((mfccs_mean, mfccs_std))\n",
    "            else:\n",
    "                 print(f\"Warning: Audio signal empty after extraction for video {i}. Using zeros.\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting audio features for video {i} ({os.path.basename(video_path)}): {e}. Using zeros.\")\n",
    "            # Ensure feature_vector remains zeros\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary audio file\n",
    "            if os.path.exists(temp_audio_path):\n",
    "                try:\n",
    "                    os.remove(temp_audio_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not remove temp audio file {temp_audio_path}: {e}\")\n",
    "\n",
    "        audio_features.append(feature_vector)\n",
    "        # print(f\"Processed audio for video {i+1}/{len(video_paths)} in {time.time() - start_time:.2f}s\") # Optional progress\n",
    "\n",
    "    # Clean up temp directory if empty\n",
    "    try:\n",
    "        if not os.listdir(temp_audio_dir):\n",
    "            os.rmdir(temp_audio_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not remove temp audio directory {temp_audio_dir}: {e}\")\n",
    "\n",
    "\n",
    "    print(\"Audio feature extraction complete.\")\n",
    "    return np.array(audio_features) # (num_trials, num_audio_features)\n",
    "\n",
    "\n",
    "# 3.3 Visual Feature Extraction (ResNet)\n",
    "def extract_visual_features(video_paths):\n",
    "    \"\"\" Extracts aggregated visual features using a pre-trained ResNet model. \"\"\"\n",
    "    print(\"Extracting Visual features (ResNet)...\")\n",
    "\n",
    "    # Load pre-trained ResNet model without the final classification layer\n",
    "    vis_model = models.resnet18(pretrained=True)\n",
    "    vis_model = nn.Sequential(*list(vis_model.children())[:-1]) # Remove the fully connected layer\n",
    "    vis_model = vis_model.to(DEVICE)\n",
    "    vis_model.eval()\n",
    "\n",
    "    # Define image transformations appropriate for ResNet\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    visual_features = []\n",
    "    num_visual_features = 512 # ResNet18 output size before FC layer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, video_path in enumerate(video_paths):\n",
    "            start_time = time.time()\n",
    "            video_feature_vector = np.zeros(num_visual_features) # Default to zeros\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                if not cap.isOpened():\n",
    "                    print(f\"Warning: Could not open video {i} ({os.path.basename(video_path)}). Using zeros.\")\n",
    "                    visual_features.append(video_feature_vector)\n",
    "                    continue\n",
    "\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if frame_count <= 0:\n",
    "                     print(f\"Warning: Video {i} has no frames or invalid frame count. Using zeros.\")\n",
    "                     visual_features.append(video_feature_vector)\n",
    "                     cap.release()\n",
    "                     continue\n",
    "\n",
    "\n",
    "                frame_indices = np.linspace(0, frame_count - 1, VISUAL_FRAMES_TO_SAMPLE, dtype=int) # Sample frames evenly\n",
    "\n",
    "                frames_data = []\n",
    "                for frame_index in frame_indices:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        # Convert frame BGR -> RGB -> PIL Image -> Apply transforms\n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        img_pil = Image.fromarray(frame_rgb)\n",
    "                        img_tensor = preprocess(img_pil).unsqueeze(0).to(DEVICE) # Add batch dimension\n",
    "                        frames_data.append(img_tensor)\n",
    "                    # else: # Optional: Warn if specific frame fails\n",
    "                        # print(f\"Warning: Could not read frame {frame_index} from video {i}.\")\n",
    "\n",
    "\n",
    "                cap.release()\n",
    "\n",
    "                if frames_data:\n",
    "                    # Stack frame tensors and pass through the model\n",
    "                    batch_tensor = torch.cat(frames_data, dim=0)\n",
    "                    frame_outputs = vis_model(batch_tensor) # (num_sampled_frames, num_visual_features, 1, 1)\n",
    "                    frame_outputs = frame_outputs.squeeze() # Remove trailing 1s -> (num_sampled_frames, num_visual_features)\n",
    "                    # Aggregate features (e.g., mean pooling)\n",
    "                    video_feature_vector = torch.mean(frame_outputs, dim=0).cpu().numpy()\n",
    "                else:\n",
    "                    print(f\"Warning: No frames could be processed for video {i}. Using zeros.\")\n",
    "                    # video_feature_vector remains zeros\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting visual features for video {i} ({os.path.basename(video_path)}): {e}. Using zeros.\")\n",
    "                if 'cap' in locals() and cap.isOpened():\n",
    "                    cap.release()\n",
    "                # video_feature_vector remains zeros\n",
    "\n",
    "            visual_features.append(video_feature_vector)\n",
    "            # print(f\"Processed visual for video {i+1}/{len(video_paths)} in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    print(\"Visual feature extraction complete.\")\n",
    "    return np.array(visual_features) # (num_trials, num_visual_features)\n",
    "\n",
    "\n",
    "# --- 4. Data Preparation for LOSO ---\n",
    "def prepare_loso(data, subject_mapping):\n",
    "    \"\"\"\n",
    "    Prepares data for LOSO cross-validation using the mandatory subject mapping.\n",
    "    Returns raw transcriptions for later NLP processing.\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for LOSO...\")\n",
    "    annotations = [item['annotation'] for item in data]\n",
    "    transcriptions = [item['transcription'] for item in data] # Keep raw text\n",
    "    video_ids = [item['video_id'] for item in data]\n",
    "\n",
    "    # Map video_ids (trial_ids) to subject IDs using the facial recognition mapping\n",
    "    mapped_subject_ids = []\n",
    "    valid_indices = [] # Keep track of trials with successful subject mapping\n",
    "    for idx, video_id in enumerate(video_ids):\n",
    "         subject_id = subject_mapping.get(video_id)\n",
    "         if subject_id is None:\n",
    "              print(f\"Critical Warning: No subject mapping found for video_id {video_id}. This trial will be skipped in prepare_loso.\")\n",
    "         else:\n",
    "              mapped_subject_ids.append(subject_id)\n",
    "              valid_indices.append(idx)\n",
    "\n",
    "    if len(valid_indices) < len(data):\n",
    "         print(f\"Warning: {len(data) - len(valid_indices)} trials were skipped due to missing subject mapping.\")\n",
    "\n",
    "    # Filter data based on valid indices\n",
    "    annotations = np.array(annotations)[valid_indices]\n",
    "    transcriptions = [transcriptions[i] for i in valid_indices]\n",
    "    mapped_subject_ids = [mapped_subject_ids[i] for i, _ in enumerate(valid_indices)] # Already filtered conceptually\n",
    "\n",
    "\n",
    "    print(f\"Data preparation complete. {len(annotations)} trials ready for LOSO.\")\n",
    "    # Note: Audio/Visual features are extracted separately AFTER prepare_loso filters trials\n",
    "    return annotations, transcriptions, mapped_subject_ids, valid_indices\n",
    "\n",
    "\n",
    "# --- 5. Multimodal Model Implementation ---\n",
    "class MultimodalDeceptionModel(nn.Module):\n",
    "    \"\"\"Enhanced multimodal model with sophisticated HSTA and NLP processing.\"\"\"\n",
    "    def __init__(self, nlp_input_size, audio_input_size, visual_input_size, hidden_size, num_classes):\n",
    "        super(MultimodalDeceptionModel, self).__init__()\n",
    "        \n",
    "        # NLP processor (unchanged)\n",
    "        self.nlp_processor = nn.Sequential(\n",
    "            nn.Linear(nlp_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Audio processor (unchanged)\n",
    "        self.audio_processor = nn.Sequential(\n",
    "            nn.Linear(audio_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Enhanced visual processor with HSTA\n",
    "        self.visual_processor = nn.Sequential(\n",
    "            nn.Linear(visual_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hsta = HierarchicalSpatioTemporalAttention(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size\n",
    "        )\n",
    "        \n",
    "        # Feature fusion with attention\n",
    "        self.fusion_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 3,\n",
    "            num_heads=8,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 3, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, nlp_data, audio_data, visual_data):\n",
    "        # Process each modality\n",
    "        nlp_processed = self.nlp_processor(nlp_data)\n",
    "        audio_processed = self.audio_processor(audio_data)\n",
    "        \n",
    "        # Process visual data with HSTA\n",
    "        visual_processed = self.visual_processor(visual_data)\n",
    "        # Reshape for HSTA (batch_size, num_frames, hidden_size)\n",
    "        batch_size = visual_processed.size(0)\n",
    "        visual_processed = visual_processed.view(batch_size, -1, visual_processed.size(-1))\n",
    "        visual_processed = self.hsta(visual_processed)\n",
    "        # Take the last frame's representation\n",
    "        visual_processed = visual_processed[:, -1, :]\n",
    "        \n",
    "        # Concatenate features\n",
    "        fused_features = torch.cat((nlp_processed, audio_processed, visual_processed), dim=1)\n",
    "        \n",
    "        # Apply fusion attention\n",
    "        fused_features = fused_features.unsqueeze(0)  # Add sequence dimension\n",
    "        fused_features, _ = self.fusion_attention(fused_features, fused_features, fused_features)\n",
    "        fused_features = fused_features.squeeze(0)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(fused_features)\n",
    "        return output\n",
    "\n",
    "# --- 6. Training and Evaluation (Modified for Multimodal) ---\n",
    "def train_evaluate(model,\n",
    "                   nlp_train, audio_train, visual_train, labels_train,\n",
    "                   nlp_test, audio_test, visual_test, labels_test,\n",
    "                   optimizer, criterion, device, epoch):\n",
    "    \"\"\" Trains and evaluates the multimodal model for one epoch. \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Move training data to device\n",
    "    nlp_train_tensor = torch.FloatTensor(nlp_train).to(device)\n",
    "    audio_train_tensor = torch.FloatTensor(audio_train).to(device)\n",
    "    visual_train_tensor = torch.FloatTensor(visual_train).to(device)\n",
    "    labels_train_tensor = torch.LongTensor(labels_train).to(device)\n",
    "\n",
    "    # Forward pass (Training)\n",
    "    outputs = model(nlp_train_tensor, audio_train_tensor, visual_train_tensor)\n",
    "    loss = criterion(outputs, labels_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move test data to device\n",
    "        nlp_test_tensor = torch.FloatTensor(nlp_test).to(device)\n",
    "        audio_test_tensor = torch.FloatTensor(audio_test).to(device)\n",
    "        visual_test_tensor = torch.FloatTensor(visual_test).to(device)\n",
    "        labels_test_tensor = torch.LongTensor(labels_test).to(device)\n",
    "\n",
    "        # Forward pass (Evaluation)\n",
    "        outputs_test = model(nlp_test_tensor, audio_test_tensor, visual_test_tensor)\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        labels_test_cpu = labels_test_tensor.cpu().numpy()\n",
    "        predicted_cpu = predicted.cpu().numpy()\n",
    "        accuracy = accuracy_score(labels_test_cpu, predicted_cpu)\n",
    "        f1 = f1_score(labels_test_cpu, predicted_cpu, average='weighted', zero_division=0) # Added zero_division\n",
    "\n",
    "    return accuracy, f1, loss.item()\n",
    "\n",
    "\n",
    "# --- 7. Run LOSO Cross-Validation (Modified for Multimodal) ---\n",
    "def run_loso(annotations, nlp_features, audio_features, visual_features, subject_ids,\n",
    "             checkpoint_dir=\"checkpoints\", num_epochs=50, learning_rate=0.001, hidden_size=128):\n",
    "    \"\"\" Runs LOSO cross-validation for the multimodal model. \"\"\"\n",
    "    print(\"Starting LOSO Cross-Validation...\")\n",
    "    loso = LeaveOneGroupOut()\n",
    "    all_accuracies = []\n",
    "    all_f1s = []\n",
    "    all_losses = []  # To store loss per fold and seed\n",
    "    num_seeds = 3 # Keep number of seeds\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    num_classes = len(np.unique(annotations))\n",
    "    if num_classes < 2:\n",
    "        print(f\"Error: Only {num_classes} unique class found. Cannot perform classification.\")\n",
    "        return\n",
    "\n",
    "    # Use subject_ids (mapped) as groups for LOSO\n",
    "    # Ensure all feature arrays have the same number of samples as annotations/subject_ids\n",
    "    n_samples = len(annotations)\n",
    "    assert len(nlp_features) == n_samples, f\"NLP features length mismatch: {len(nlp_features)} vs {n_samples}\"\n",
    "    assert len(audio_features) == n_samples, f\"Audio features length mismatch: {len(audio_features)} vs {n_samples}\"\n",
    "    assert len(visual_features) == n_samples, f\"Visual features length mismatch: {len(visual_features)} vs {n_samples}\"\n",
    "    assert len(subject_ids) == n_samples, f\"Subject IDs length mismatch: {len(subject_ids)} vs {n_samples}\"\n",
    "\n",
    "\n",
    "    fold_num = 0\n",
    "    for train_index, test_index in loso.split(nlp_features, annotations, groups=subject_ids):\n",
    "        fold_num += 1\n",
    "        fold_accuracies_seeds = []\n",
    "        fold_f1s_seeds = []\n",
    "        fold_losses_seeds = {} # Store losses per seed {seed: [epoch_losses]}\n",
    "\n",
    "        test_subject = np.unique(np.array(subject_ids)[test_index])[0]\n",
    "        print(f\"\\n--- Fold {fold_num}/{loso.get_n_splits(groups=subject_ids)}: Testing on Subject {test_subject} ---\")\n",
    "\n",
    "\n",
    "        # Split data for this fold\n",
    "        nlp_train, nlp_test = nlp_features[train_index], nlp_features[test_index]\n",
    "        audio_train, audio_test = audio_features[train_index], audio_features[test_index]\n",
    "        visual_train, visual_test = visual_features[train_index], visual_features[test_index]\n",
    "        labels_train, labels_test = annotations[train_index], annotations[test_index]\n",
    "\n",
    "        # Check if train or test set is empty for this fold (can happen with LOSO if subject has few samples)\n",
    "        if len(labels_train) == 0 or len(labels_test) == 0:\n",
    "             print(f\"Warning: Skipping Fold {fold_num} due to empty train ({len(labels_train)}) or test ({len(labels_test)}) set.\")\n",
    "             continue\n",
    "\n",
    "        # Get feature dimensions dynamically\n",
    "        nlp_dim = nlp_train.shape[1]\n",
    "        audio_dim = audio_train.shape[1]\n",
    "        visual_dim = visual_train.shape[1]\n",
    "\n",
    "        for seed in range(num_seeds):\n",
    "            print(f\"  Seed {seed + 1}/{num_seeds}\")\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed) # Also seed numpy if any random operations happen there\n",
    "\n",
    "            # Model initialization, optimizer, and loss function\n",
    "            model = MultimodalDeceptionModel(\n",
    "                nlp_input_size=nlp_dim,\n",
    "                audio_input_size=audio_dim,\n",
    "                visual_input_size=visual_dim,\n",
    "                hidden_size=hidden_size,\n",
    "                num_classes=num_classes\n",
    "            ).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "            # Define checkpoint file name\n",
    "            checkpoint_file = os.path.join(\n",
    "                checkpoint_dir, f\"fold_{fold_num}_seed_{seed + 1}.pth\")\n",
    "\n",
    "            # Check if checkpoint exists and load it\n",
    "            start_epoch = 0\n",
    "            if os.path.exists(checkpoint_file):\n",
    "                try:\n",
    "                    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    start_epoch = checkpoint['epoch']\n",
    "                    last_loss = checkpoint.get('loss', 'N/A') # Get last saved loss if available\n",
    "                    print(f\"    Resuming training from checkpoint {checkpoint_file} at epoch {start_epoch} (Last saved loss: {last_loss})\")\n",
    "                except Exception as e:\n",
    "                     print(f\"    Warning: Could not load checkpoint {checkpoint_file}: {e}. Starting from scratch.\")\n",
    "                     start_epoch = 0\n",
    "\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            epoch_losses = []\n",
    "            best_f1_seed = -1.0 # Track best F1 for this seed run\n",
    "\n",
    "            for epoch in range(start_epoch, num_epochs):\n",
    "                accuracy, f1, loss = train_evaluate(\n",
    "                    model,\n",
    "                    nlp_train, audio_train, visual_train, labels_train,\n",
    "                    nlp_test, audio_test, visual_test, labels_test,\n",
    "                    optimizer, criterion, DEVICE, epoch)\n",
    "\n",
    "                epoch_losses.append(loss)\n",
    "\n",
    "                # Optional: Save checkpoint only if F1 improves for this seed\n",
    "                if f1 > best_f1_seed:\n",
    "                     best_f1_seed = f1\n",
    "                     torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'f1': f1, # Save F1 score in checkpoint\n",
    "                        'accuracy': accuracy,\n",
    "                     }, checkpoint_file)\n",
    "                     # print(f\"      Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f}, Acc: {accuracy:.4f}, F1: {f1:.4f} (Checkpoint Saved)\")\n",
    "                #else:\n",
    "                     # print(f\"      Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f}, Acc: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "\n",
    "            # Store results from the *last* epoch for this seed\n",
    "            # (Alternatively, load the best checkpoint and evaluate on that)\n",
    "            # For simplicity, using last epoch results here:\n",
    "            fold_accuracies_seeds.append(accuracy)\n",
    "            fold_f1s_seeds.append(f1)\n",
    "            fold_losses_seeds[seed + 1] = epoch_losses\n",
    "\n",
    "\n",
    "        # Average metrics across seeds for this fold\n",
    "        if fold_accuracies_seeds: # Check if any seeds ran successfully\n",
    "             avg_fold_accuracy = np.mean(fold_accuracies_seeds)\n",
    "             avg_fold_f1 = np.mean(fold_f1s_seeds)\n",
    "             all_accuracies.append(avg_fold_accuracy)\n",
    "             all_f1s.append(avg_fold_f1)\n",
    "             # Storing all epoch losses per seed per fold can be large - maybe just avg loss?\n",
    "             # For now, let's store the list of losses for the last epoch of each seed\n",
    "             last_epoch_losses = [losses[-1] for losses in fold_losses_seeds.values() if losses]\n",
    "             all_losses.append(np.mean(last_epoch_losses) if last_epoch_losses else float('nan'))\n",
    "\n",
    "\n",
    "             print(f\"  Fold {fold_num} Average (across seeds) - Accuracy: {avg_fold_accuracy:.4f}, F1: {avg_fold_f1:.4f}\")\n",
    "        else:\n",
    "             print(f\"  Fold {fold_num} - No successful seed runs.\")\n",
    "\n",
    "\n",
    "    # Overall results across folds\n",
    "    if all_accuracies:\n",
    "         overall_avg_accuracy = np.mean(all_accuracies)\n",
    "         overall_avg_f1 = np.mean(all_f1s)\n",
    "         print(f\"\\n--- Overall LOSO Results ---\")\n",
    "         print(f\"Overall Average Accuracy: {overall_avg_accuracy:.4f}\")\n",
    "         print(f\"Overall Average F1-score: {overall_avg_f1:.4f}\")\n",
    "         # print(f\"Average last epoch losses across folds: {all_losses}\")\n",
    "    else:\n",
    "         print(\"\\n--- No folds completed successfully. Cannot calculate overall results. ---\")\n",
    "\n",
    "#--- 8. HierarchicalSpatioTemporalAttention ---\n",
    "class HierarchicalSpatioTemporalAttention(nn.Module):\n",
    "    \"\"\"Hierarchical Spatio-Temporal Attention module for video processing.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_heads=8, dropout=0.1):\n",
    "        super(HierarchicalSpatioTemporalAttention, self).__init__()\n",
    "        \n",
    "        # Multi-head attention for spatial features\n",
    "        self.spatial_attention = nn.MultiheadAttention(\n",
    "            embed_dim=input_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Temporal attention layers for different scales\n",
    "        self.temporal_attention1 = nn.MultiheadAttention(\n",
    "            embed_dim=input_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.temporal_attention2 = nn.MultiheadAttention(\n",
    "            embed_dim=input_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal encoding\n",
    "        self.temporal_encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(input_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_size)\n",
    "        self.layer_norm3 = nn.LayerNorm(input_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Projection layers\n",
    "        self.projection = nn.Linear(hidden_size * 2, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, num_features)\n",
    "        batch_size, num_frames, num_features = x.size()\n",
    "        \n",
    "        # Reshape for spatial attention\n",
    "        spatial_x = x.view(batch_size * num_frames, 1, num_features)\n",
    "        spatial_x = spatial_x.transpose(0, 1)  # (1, batch_size * num_frames, num_features)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        spatial_out, _ = self.spatial_attention(spatial_x, spatial_x, spatial_x)\n",
    "        spatial_out = spatial_out.transpose(0, 1)  # (batch_size * num_frames, 1, num_features)\n",
    "        spatial_out = spatial_out.view(batch_size, num_frames, num_features)\n",
    "        spatial_out = self.layer_norm1(x + self.dropout(spatial_out))\n",
    "        \n",
    "        # First temporal scale (original frame rate)\n",
    "        temporal_out1, _ = self.temporal_attention1(\n",
    "            spatial_out.transpose(0, 1),\n",
    "            spatial_out.transpose(0, 1),\n",
    "            spatial_out.transpose(0, 1)\n",
    "        )\n",
    "        temporal_out1 = temporal_out1.transpose(0, 1)\n",
    "        temporal_out1 = self.layer_norm2(spatial_out + self.dropout(temporal_out1))\n",
    "        \n",
    "        # Second temporal scale (downsampled)\n",
    "        if num_frames > 1:\n",
    "            downsampled = temporal_out1[:, ::2, :]  # Simple downsampling\n",
    "            temporal_out2, _ = self.temporal_attention2(\n",
    "                downsampled.transpose(0, 1),\n",
    "                downsampled.transpose(0, 1),\n",
    "                downsampled.transpose(0, 1)\n",
    "            )\n",
    "            temporal_out2 = temporal_out2.transpose(0, 1)\n",
    "            temporal_out2 = self.layer_norm3(downsampled + self.dropout(temporal_out2))\n",
    "            \n",
    "            # Upsample and combine\n",
    "            temporal_out2 = F.interpolate(\n",
    "                temporal_out2.transpose(1, 2),\n",
    "                size=num_frames,\n",
    "                mode='linear',\n",
    "                align_corners=False\n",
    "            ).transpose(1, 2)\n",
    "            temporal_out = temporal_out1 + temporal_out2\n",
    "        else:\n",
    "            temporal_out = temporal_out1\n",
    "        \n",
    "        # Final temporal encoding with LSTM\n",
    "        lstm_out, _ = self.temporal_encoder(temporal_out)\n",
    "        lstm_out = self.projection(lstm_out)\n",
    "        \n",
    "        return lstm_out\n",
    "\n",
    "\n",
    "\n",
    "# --- 9. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    start_main_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # !!! ADJUST THESE PATHS TO YOUR DATASET LOCATION !!!\n",
    "    data_dir = 'Real-life_Deception_Detection_2016' # Example path\n",
    "    annotation_file = \"Real-life_Deception_Detection_2016\\Annotation\\All_Gestures_Deceptive and Truthful.csv\"\n",
    "    checkpoint_dir = \"multimodal_checkpoints\"\n",
    "    num_epochs_main = 50  # Adjust number of epochs\n",
    "    learning_rate_main = 0.001\n",
    "    hidden_size_main = 128 # Hidden dimension for feature processing/fusion\n",
    "\n",
    "    # --- Workflow ---\n",
    "    # 1. Load Data (Paths, Annotations, Transcriptions)\n",
    "    data = load_data(data_dir, annotation_file)\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data loaded. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Identify Subjects (Mandatory Facial Recognition)\n",
    "    subject_mapping = identify_subjects_facial_recognition(data)\n",
    "    # print(\"Subject Mapping (Facial Recognition):\", subject_mapping) # Optional print\n",
    "\n",
    "    # 3. Prepare Data for LOSO (Get filtered annotations, raw transcriptions, mapped IDs)\n",
    "    annotations, transcriptions_raw, mapped_subject_ids, valid_indices = prepare_loso(data, subject_mapping)\n",
    "\n",
    "    if len(annotations) == 0:\n",
    "        print(\"No valid trials remaining after preparing for LOSO. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Filter original data list based on valid indices from prepare_loso\n",
    "    # This ensures feature extraction only happens for trials included in LOSO\n",
    "    valid_data = [data[i] for i in valid_indices]\n",
    "    video_paths_valid = [item['video_path'] for item in valid_data]\n",
    "\n",
    "    # 4. Extract Features for the valid trials\n",
    "    nlp_features = extract_nlp_features(transcriptions_raw) # Takes raw text\n",
    "    audio_features = extract_audio_features(video_paths_valid)\n",
    "    visual_features = extract_visual_features(video_paths_valid)\n",
    "\n",
    "    # Sanity check feature shapes before running LOSO\n",
    "    print(f\"Feature shapes: NLP={nlp_features.shape}, Audio={audio_features.shape}, Visual={visual_features.shape}\")\n",
    "    if not (nlp_features.shape[0] == audio_features.shape[0] == visual_features.shape[0] == len(annotations)):\n",
    "         print(\"Error: Feature array lengths do not match number of annotations after filtering. Exiting.\")\n",
    "         print(f\"Lengths: Annotations={len(annotations)}, NLP={nlp_features.shape[0]}, Audio={audio_features.shape[0]}, Visual={visual_features.shape[0]}\")\n",
    "         exit()\n",
    "\n",
    "\n",
    "    # 5. Run LOSO Cross-Validation with Multimodal Features\n",
    "    run_loso(annotations, nlp_features, audio_features, visual_features, mapped_subject_ids,\n",
    "             checkpoint_dir=checkpoint_dir,\n",
    "             num_epochs=num_epochs_main,\n",
    "             learning_rate=learning_rate_main,\n",
    "             hidden_size=hidden_size_main)\n",
    "\n",
    "    end_main_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {(end_main_time - start_main_time) / 60:.2f} minutes\")\n",
    "    print(\"Multimodal training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
