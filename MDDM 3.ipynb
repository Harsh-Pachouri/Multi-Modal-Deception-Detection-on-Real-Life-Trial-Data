{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--- Starting Deception Detection Pipeline ---\n",
      "Dataset Directory: d:\\Coding\\Deception Detection\\Real-life_Deception_Detection_2016\n",
      "Annotation File: d:\\Coding\\Deception Detection\\Real-life_Deception_Detection_2016\\Annotation\\All_Gestures_Deceptive and Truthful.csv\n",
      "Checkpoints Directory: d:\\Coding\\Deception Detection\\multimodal_checkpoints_simple\n",
      "Device: cpu\n",
      "Number of Epochs: 50\n",
      "Learning Rate: 0.001\n",
      "Hidden Size: 128\n",
      "Visual Frames to Sample: 70\n",
      "Audio MFCCs: 13\n",
      "-------------------------------------------\n",
      "Loading data...\n",
      "Found 121 video files\n",
      "Found 121 transcription files\n",
      "Data loading complete. Found 121 synchronized trials.\n",
      "Starting mandatory facial recognition for subject identification...\n",
      "Facial recognition complete. Identified 44 unique subjects and 12 videos needing unique IDs.\n",
      "Preparing data for LOSO...\n",
      "Data preparation complete. 121 trials ready for LOSO.\n",
      "Extracting NLP features (BERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP feature extraction complete.\n",
      "Extracting Audio features (MFCCs)...\n",
      "Audio feature extraction complete.\n",
      "Extracting Visual features (ResNet)...\n",
      "Visual feature extraction complete.\n",
      "Feature shapes after extraction: NLP=(121, 768), Audio=(121, 26), Visual=(121, 512)\n",
      "Starting LOSO Cross-Validation...\n",
      "\n",
      "--- Fold 1/56: Testing on Subject(s) subject_1 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.7500, F1: 0.6947\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.7500, F1: 0.7556\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 0.6667, F1: 0.6762\n",
      "  Fold 1 Average (across seeds) - Accuracy: 0.7222, F1: 0.7088, Last Epoch Loss: 0.4949\n",
      "\n",
      "--- Fold 2/56: Testing on Subject(s) subject_10 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.8333, F1: 0.8381\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.8333, F1: 0.8381\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 0.6667, F1: 0.5333\n",
      "  Fold 2 Average (across seeds) - Accuracy: 0.7778, F1: 0.7365, Last Epoch Loss: 0.5212\n",
      "\n",
      "--- Fold 3/56: Testing on Subject(s) subject_11 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 3 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5062\n",
      "\n",
      "--- Fold 4/56: Testing on Subject(s) subject_12 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 4 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5182\n",
      "\n",
      "--- Fold 5/56: Testing on Subject(s) subject_13 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 5 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.6264\n",
      "\n",
      "--- Fold 6/56: Testing on Subject(s) subject_14 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 6 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5199\n",
      "\n",
      "--- Fold 7/56: Testing on Subject(s) subject_15 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 7 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5312\n",
      "\n",
      "--- Fold 8/56: Testing on Subject(s) subject_16 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 8 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5065\n",
      "\n",
      "--- Fold 9/56: Testing on Subject(s) subject_17 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 9 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5318\n",
      "\n",
      "--- Fold 10/56: Testing on Subject(s) subject_18 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.0000, F1: 0.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 10 Average (across seeds) - Accuracy: 0.6667, F1: 0.6667, Last Epoch Loss: 0.5194\n",
      "\n",
      "--- Fold 11/56: Testing on Subject(s) subject_19 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 11 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5267\n",
      "\n",
      "--- Fold 12/56: Testing on Subject(s) subject_2 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.8000, F1: 0.8000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.7000, F1: 0.6901\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 0.7000, F1: 0.7030\n",
      "  Fold 12 Average (across seeds) - Accuracy: 0.7333, F1: 0.7310, Last Epoch Loss: 0.4281\n",
      "\n",
      "--- Fold 13/56: Testing on Subject(s) subject_20 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 13 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5107\n",
      "\n",
      "--- Fold 14/56: Testing on Subject(s) subject_21 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 14 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5138\n",
      "\n",
      "--- Fold 15/56: Testing on Subject(s) subject_22 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 15 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5199\n",
      "\n",
      "--- Fold 16/56: Testing on Subject(s) subject_23 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 16 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5507\n",
      "\n",
      "--- Fold 17/56: Testing on Subject(s) subject_24 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 17 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.6305\n",
      "\n",
      "--- Fold 18/56: Testing on Subject(s) subject_25 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 18 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5566\n",
      "\n",
      "--- Fold 19/56: Testing on Subject(s) subject_26 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 19 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5320\n",
      "\n",
      "--- Fold 20/56: Testing on Subject(s) subject_27 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 20 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5248\n",
      "\n",
      "--- Fold 21/56: Testing on Subject(s) subject_28 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 21 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5433\n",
      "\n",
      "--- Fold 22/56: Testing on Subject(s) subject_29 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 22 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5540\n",
      "\n",
      "--- Fold 23/56: Testing on Subject(s) subject_3 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 23 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5151\n",
      "\n",
      "--- Fold 24/56: Testing on Subject(s) subject_30 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 24 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5400\n",
      "\n",
      "--- Fold 25/56: Testing on Subject(s) subject_31 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 25 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5368\n",
      "\n",
      "--- Fold 26/56: Testing on Subject(s) subject_32 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 26 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5487\n",
      "\n",
      "--- Fold 27/56: Testing on Subject(s) subject_33 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 27 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5183\n",
      "\n",
      "--- Fold 28/56: Testing on Subject(s) subject_34 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 28 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5695\n",
      "\n",
      "--- Fold 29/56: Testing on Subject(s) subject_35 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 29 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5406\n",
      "\n",
      "--- Fold 30/56: Testing on Subject(s) subject_36 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 30 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5340\n",
      "\n",
      "--- Fold 31/56: Testing on Subject(s) subject_37 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 31 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5319\n",
      "\n",
      "--- Fold 32/56: Testing on Subject(s) subject_38 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 32 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5042\n",
      "\n",
      "--- Fold 33/56: Testing on Subject(s) subject_39 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 33 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5266\n",
      "\n",
      "--- Fold 34/56: Testing on Subject(s) subject_4 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.8571, F1: 0.8629\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.8571, F1: 0.8201\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 0.8571, F1: 0.8201\n",
      "  Fold 34 Average (across seeds) - Accuracy: 0.8571, F1: 0.8344, Last Epoch Loss: 0.4209\n",
      "\n",
      "--- Fold 35/56: Testing on Subject(s) subject_40 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 35 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5415\n",
      "\n",
      "--- Fold 36/56: Testing on Subject(s) subject_41 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 36 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5310\n",
      "\n",
      "--- Fold 37/56: Testing on Subject(s) subject_42 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 37 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5303\n",
      "\n",
      "--- Fold 38/56: Testing on Subject(s) subject_43 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 38 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5282\n",
      "\n",
      "--- Fold 39/56: Testing on Subject(s) subject_44 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 39 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5349\n",
      "\n",
      "--- Fold 40/56: Testing on Subject(s) subject_5 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 40 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.4897\n",
      "\n",
      "--- Fold 41/56: Testing on Subject(s) subject_6 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 41 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.4890\n",
      "\n",
      "--- Fold 42/56: Testing on Subject(s) subject_7 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.0000, F1: 0.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.0000, F1: 0.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 42 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333, Last Epoch Loss: 0.5017\n",
      "\n",
      "--- Fold 43/56: Testing on Subject(s) subject_8 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.0000, F1: 0.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 0.0000, F1: 0.0000\n",
      "  Fold 43 Average (across seeds) - Accuracy: 0.3333, F1: 0.3333, Last Epoch Loss: 0.5145\n",
      "\n",
      "--- Fold 44/56: Testing on Subject(s) subject_9 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 0.5000, F1: 0.3333\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 0.5000, F1: 0.3333\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 0.5000, F1: 0.3333\n",
      "  Fold 44 Average (across seeds) - Accuracy: 0.5000, F1: 0.3333, Last Epoch Loss: 0.5169\n",
      "\n",
      "--- Fold 45/56: Testing on Subject(s) unknown_no_face_1 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 45 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.4986\n",
      "\n",
      "--- Fold 46/56: Testing on Subject(s) unknown_no_face_10 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 46 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5243\n",
      "\n",
      "--- Fold 47/56: Testing on Subject(s) unknown_no_face_11 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 47 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5215\n",
      "\n",
      "--- Fold 48/56: Testing on Subject(s) unknown_no_face_12 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 48 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5359\n",
      "\n",
      "--- Fold 49/56: Testing on Subject(s) unknown_no_face_2 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 49 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5168\n",
      "\n",
      "--- Fold 50/56: Testing on Subject(s) unknown_no_face_3 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 50 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5001\n",
      "\n",
      "--- Fold 51/56: Testing on Subject(s) unknown_no_face_4 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 51 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5287\n",
      "\n",
      "--- Fold 52/56: Testing on Subject(s) unknown_no_face_5 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 52 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5338\n",
      "\n",
      "--- Fold 53/56: Testing on Subject(s) unknown_no_face_6 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 53 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5201\n",
      "\n",
      "--- Fold 54/56: Testing on Subject(s) unknown_no_face_7 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 54 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5468\n",
      "\n",
      "--- Fold 55/56: Testing on Subject(s) unknown_no_face_8 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 55 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5379\n",
      "\n",
      "--- Fold 56/56: Testing on Subject(s) unknown_no_face_9 ---\n",
      "  Seed 1/3\n",
      "    Seed 1 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 2/3\n",
      "    Seed 2 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Seed 3/3\n",
      "    Seed 3 Final Results (Best F1 Model) - Accuracy: 1.0000, F1: 1.0000\n",
      "  Fold 56 Average (across seeds) - Accuracy: 1.0000, F1: 1.0000, Last Epoch Loss: 0.5315\n",
      "\n",
      "--- Overall LOSO Results (Avg. Across Folds & Seeds) ---\n",
      "Overall Average Accuracy: 0.9451\n",
      "Overall Average F1-score: 0.9407\n",
      "Overall Average Last Epoch Loss: 0.5255\n",
      "\n",
      "Total execution time: 10.63 minutes\n",
      "Multimodal training complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pre-process-3: Combined and Refined Multimodal Deception Detection Script\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import re\n",
    "import cv2 # OpenCV for video processing\n",
    "import face_recognition # Mandatory for subject identification\n",
    "import librosa # For audio analysis\n",
    "from moviepy.editor import VideoFileClip # For extracting audio\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "import torch.nn.functional as F # Needed for interpolate if using HSTA, but not used in this simpler model\n",
    "\n",
    "# Suppress warnings from libraries like moviepy/librosa if needed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "VISUAL_MODEL_NAME = 'resnet18' # Using ResNet18 for visual features\n",
    "AUDIO_N_MFCC = 13 # Number of MFCCs for audio features\n",
    "VISUAL_FRAMES_TO_SAMPLE = 70 # Increased frames back to version 1's value [cite: 491]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- 1. Subject Identification (Mandatory Facial Recognition) ---\n",
    "# Using the robust function from pre-process-2 [cite: 3, 495]\n",
    "def identify_subjects_facial_recognition(data):\n",
    "    \"\"\"\n",
    "    Identifies subjects MANDATORILY using facial recognition from the first frame.\n",
    "    Maps trial_id to a subject label.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of dictionaries from load_data, containing\n",
    "                     'video_path' and 'video_id' (trial_id). [cite: 4]\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping trial_ids (video_ids) to subject labels\n",
    "              (e.g., 'subject_1', 'unknown_trial_xyz'). [cite: 5, 494]\n",
    "    \"\"\"\n",
    "    print(\"Starting mandatory facial recognition for subject identification...\")\n",
    "    subject_mapping = {}\n",
    "    known_faces = {} # Store known face encodings and labels {label: encoding}\n",
    "    subject_counter = 1\n",
    "    unknown_counter = 1\n",
    "\n",
    "    for item in data:\n",
    "        video_path = item['video_path']\n",
    "        trial_id = item['video_id'] # Use the actual trial_id\n",
    "        subject_label = None\n",
    "\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Warning: Could not open video file {video_path} for trial {trial_id}. Assigning unknown subject.\")\n",
    "                subject_label = f'unknown_video_open_error_{unknown_counter}'\n",
    "                unknown_counter += 1\n",
    "                subject_mapping[trial_id] = subject_label\n",
    "                continue\n",
    "\n",
    "            # Read the first frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Warning: Could not read frame from video {video_path} for trial {trial_id}. Assigning unknown subject.\")\n",
    "                subject_label = f'unknown_frame_read_error_{unknown_counter}'\n",
    "                unknown_counter += 1\n",
    "                subject_mapping[trial_id] = subject_label\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            # Convert the frame to RGB (face_recognition uses RGB)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Find face locations and encodings in the frame [cite: 7, 496]\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            if not face_encodings:\n",
    "                #print(f\"Warning: No faces found in the first frame of video {trial_id}. Assigning unique unknown subject.\")\n",
    "                subject_label = f'unknown_no_face_{unknown_counter}' # Assign a unique unknown label\n",
    "                unknown_counter += 1\n",
    "            else:\n",
    "                # Use the first face found\n",
    "                current_face_encoding = face_encodings[0]\n",
    "\n",
    "                # Check for matches with known faces\n",
    "                match_found = False\n",
    "                known_labels = list(known_faces.keys())\n",
    "                if known_labels:\n",
    "                    known_encodings = list(known_faces.values())\n",
    "                    # Increase tolerance slightly if needed, default is 0.6\n",
    "                    matches = face_recognition.compare_faces(known_encodings, current_face_encoding, tolerance=0.6)\n",
    "\n",
    "                    # Find the first match\n",
    "                    try:\n",
    "                        first_match_index = matches.index(True)\n",
    "                        subject_label = known_labels[first_match_index]\n",
    "                        match_found = True\n",
    "                    except ValueError: # No True value found in matches\n",
    "                        pass\n",
    "\n",
    "                if not match_found:\n",
    "                    # If no match, add the face to known faces\n",
    "                    subject_label = f'subject_{subject_counter}'\n",
    "                    known_faces[subject_label] = current_face_encoding\n",
    "                    subject_counter += 1\n",
    "\n",
    "            subject_mapping[trial_id] = subject_label\n",
    "            cap.release()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path} for trial {trial_id}: {e}. Assigning unknown subject.\")\n",
    "            subject_label = f'unknown_processing_error_{unknown_counter}'\n",
    "            unknown_counter += 1\n",
    "            subject_mapping[trial_id] = subject_label\n",
    "            if 'cap' in locals() and cap.isOpened():\n",
    "                cap.release()\n",
    "\n",
    "    print(f\"Facial recognition complete. Identified {subject_counter - 1} unique subjects and {unknown_counter - 1} videos needing unique IDs.\")\n",
    "    return subject_mapping\n",
    "\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "# Using the refined function from pre-process-2 [cite: 9, 499] that reads .txt transcriptions\n",
    "def load_data(data_dir, annotation_file):\n",
    "    \"\"\"Loads and synchronizes annotation, transcription, and video data.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    # Construct paths\n",
    "    clip_dirs = [\n",
    "        os.path.join(data_dir, 'Clips', 'Deceptive'),\n",
    "        os.path.join(data_dir, 'Clips', 'Truthful')\n",
    "    ]\n",
    "    transcript_dirs = [\n",
    "        os.path.join(data_dir, 'Transcription', 'Deceptive'),\n",
    "        os.path.join(data_dir, 'Transcription', 'Truthful')\n",
    "    ]\n",
    "\n",
    "    # Initialize lists to store paths\n",
    "    video_paths = []\n",
    "    transcription_paths = []\n",
    "\n",
    "    # Load video paths\n",
    "    for clip_dir in clip_dirs:\n",
    "        if os.path.isdir(clip_dir):\n",
    "            for filename in os.listdir(clip_dir):\n",
    "                if filename.endswith(\".mp4\"):\n",
    "                    video_paths.append(os.path.join(clip_dir, filename))\n",
    "        else:\n",
    "            print(f\"Warning: Clip directory not found: {clip_dir}\")\n",
    "\n",
    "    # Load transcription paths (looking for .txt files) [cite: 10, 499]\n",
    "    for transcript_dir in transcript_dirs:\n",
    "        if os.path.isdir(transcript_dir):\n",
    "            for filename in os.listdir(transcript_dir):\n",
    "                if filename.endswith(\".txt\"): # Changed from .csv to .txt\n",
    "                    transcription_paths.append(os.path.join(transcript_dir, filename))\n",
    "        else:\n",
    "            print(f\"Warning: Transcription directory not found: {transcript_dir}\")\n",
    "\n",
    "    # Load annotations\n",
    "    try:\n",
    "        annotations_df = pd.read_csv(annotation_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Annotation file not found at {annotation_file}\")\n",
    "        return []\n",
    "\n",
    "    # Create mapping dictionaries\n",
    "    video_path_dict = {}\n",
    "    for video_path in video_paths:\n",
    "        video_filename = os.path.basename(video_path)\n",
    "        # Extract trial_id like 'trial_lie_001' or 'trial_truth_001'\n",
    "        match = re.search(r\"trial_(truth|lie)_(\\d+)\\.mp4\", video_filename)\n",
    "        if match:\n",
    "            trial_id = f\"trial_{match.group(1)}_{match.group(2)}\"\n",
    "            video_path_dict[trial_id] = video_path\n",
    "\n",
    "    print(f\"Found {len(video_paths)} video files\")\n",
    "    print(f\"Found {len(transcription_paths)} transcription files\")\n",
    "\n",
    "    transcription_dict = {}\n",
    "    for transcript_path in transcription_paths:\n",
    "        try:\n",
    "            # Read the .txt file directly [cite: 11, 500]\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                transcription_text = f.read().strip()\n",
    "\n",
    "            # Extract trial_id from filename\n",
    "            filename = os.path.basename(transcript_path)\n",
    "            trial_id = filename.replace('.txt', '') # Remove .txt extension\n",
    "            # print(f\"Reading transcription file: {transcript_path}\") # Optional Debug\n",
    "            transcription_dict[trial_id] = transcription_text\n",
    "            # print(f\"Added transcription for {trial_id}\") # Optional Debug\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading transcription file {transcript_path}: {e}\")\n",
    "\n",
    "    # Synchronize data\n",
    "    synchronized_data = []\n",
    "    processed_ids = set()\n",
    "\n",
    "    if 'id' not in annotations_df.columns or 'class' not in annotations_df.columns:\n",
    "        print(f\"Error: Annotation file missing 'id' or 'class' column.\")\n",
    "        return []\n",
    "\n",
    "    for _, row in annotations_df.iterrows(): # Use _ to ignore index\n",
    "        # Extract trial_id from annotation 'id' column, assuming format like 'trial_lie_001.mp4'\n",
    "        trial_id_raw = str(row['id'])\n",
    "        match = re.search(r\"trial_(truth|lie)_(\\d+)\", trial_id_raw)\n",
    "        if not match:\n",
    "             print(f\"Warning: Skipping annotation row with unexpected id format: {trial_id_raw}\")\n",
    "             continue\n",
    "        trial_id = f\"trial_{match.group(1)}_{match.group(2)}\"\n",
    "\n",
    "        annotation_label = row['class']\n",
    "\n",
    "        if annotation_label not in ['truthful', 'deceptive']:\n",
    "            print(f\"Warning: Skipping trial {trial_id} due to unexpected class label: {annotation_label}\")\n",
    "            continue\n",
    "\n",
    "        if trial_id in processed_ids:\n",
    "            print(f\"Warning: Duplicate trial ID {trial_id} found in annotations. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        transcription_text = transcription_dict.get(trial_id)\n",
    "        video_path = video_path_dict.get(trial_id)\n",
    "\n",
    "        if transcription_text is None:\n",
    "            print(f\"Warning: Transcription not found for trial {trial_id}. Skipping trial.\")\n",
    "            continue\n",
    "\n",
    "        if video_path is None:\n",
    "            print(f\"Warning: Video path not found for trial {trial_id}. Skipping trial.\")\n",
    "            continue\n",
    "\n",
    "        # Map labels to numerical values\n",
    "        label_map = {'truthful': 0, 'deceptive': 1}\n",
    "        numeric_label = label_map.get(annotation_label)\n",
    "\n",
    "        if numeric_label is None: # Should not happen with the check above, but good practice\n",
    "            print(f\"Warning: Could not map label '{annotation_label}' for trial {trial_id}. Skipping trial.\")\n",
    "            continue\n",
    "\n",
    "        synchronized_data.append({\n",
    "            'annotation': numeric_label,\n",
    "            'transcription': transcription_text,\n",
    "            'video_id': trial_id,\n",
    "            'video_path': video_path\n",
    "        })\n",
    "        processed_ids.add(trial_id)\n",
    "\n",
    "    print(f\"Data loading complete. Found {len(synchronized_data)} synchronized trials.\")\n",
    "    return synchronized_data\n",
    "\n",
    "\n",
    "# --- 3. Feature Extraction ---\n",
    "\n",
    "# 3.1 NLP Feature Extraction (BERT) - Unchanged from pre-process-2 [cite: 15, 503]\n",
    "def extract_nlp_features(transcriptions):\n",
    "    \"\"\" Extracts BERT embeddings for a list of transcriptions. \"\"\"\n",
    "    print(\"Extracting NLP features (BERT)...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "    model = BertModel.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
    "    model.eval()\n",
    "    nlp_features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, text in enumerate(transcriptions):\n",
    "            try:\n",
    "                # Ensure text is a string\n",
    "                text = str(text) if text is not None else \"\"\n",
    "                if not text.strip(): # Handle empty strings\n",
    "                    print(f\"Warning: Empty transcription for item {i}. Using zero vector.\")\n",
    "                    # Get expected hidden size from model config\n",
    "                    hidden_size = model.config.hidden_size\n",
    "                    sentence_embedding = np.zeros((1, hidden_size))\n",
    "                else:\n",
    "                    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(DEVICE) # Added max_length\n",
    "                    outputs = model(**inputs)\n",
    "                    # Mean of the last hidden state\n",
    "                    sentence_embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy() # (1, hidden_size)\n",
    "\n",
    "                nlp_features.append(sentence_embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting NLP features for item {i}: {e}. Using zero vector.\")\n",
    "                hidden_size = model.config.hidden_size\n",
    "                nlp_features.append(np.zeros((1, hidden_size)))\n",
    "\n",
    "    print(\"NLP feature extraction complete.\")\n",
    "\n",
    "    # Ensure all features are arrays and handle potential shape issues before stacking\n",
    "    processed_features = []\n",
    "    target_shape = None\n",
    "    if nlp_features: # Find target shape from the first valid feature\n",
    "         for feat in nlp_features:\n",
    "             if isinstance(feat, np.ndarray):\n",
    "                 target_shape = feat.shape\n",
    "                 break\n",
    "\n",
    "    if target_shape is None and nlp_features: # If no valid features found but list not empty\n",
    "        # Fallback: try to get shape from model config if possible\n",
    "        try:\n",
    "             target_shape = (1, model.config.hidden_size)\n",
    "             print(f\"Warning: Could not determine NLP target shape from features, using default BERT hidden size: {target_shape}\")\n",
    "        except Exception:\n",
    "             print(\"Error: Cannot determine NLP feature shape. Returning empty array.\")\n",
    "             return np.array([]) # Cannot proceed without a shape\n",
    "    elif not nlp_features:\n",
    "        print(\"Warning: No NLP features extracted.\")\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "    for feat in nlp_features:\n",
    "        if isinstance(feat, np.ndarray):\n",
    "            if feat.shape == target_shape:\n",
    "                processed_features.append(feat)\n",
    "            else:\n",
    "                # If shape mismatch, pad or truncate (or use zeros as done in exception handling)\n",
    "                print(f\"Warning: NLP feature shape mismatch ({feat.shape} vs {target_shape}). Using zero vector of target shape.\")\n",
    "                processed_features.append(np.zeros(target_shape))\n",
    "        else: # Should not happen if exceptions are caught, but as safeguard\n",
    "            print(f\"Warning: Non-array NLP feature found. Using zero vector of target shape.\")\n",
    "            processed_features.append(np.zeros(target_shape))\n",
    "\n",
    "\n",
    "    if not processed_features:\n",
    "        return np.array([]) # Return empty array if no features were processed\n",
    "\n",
    "    return np.vstack(processed_features) # (num_trials, hidden_size)\n",
    "\n",
    "\n",
    "# 3.2 Audio Feature Extraction (MFCCs) - Unchanged from pre-process-2 [cite: 18, 506]\n",
    "def extract_audio_features(video_paths):\n",
    "    \"\"\" Extracts MFCC features from the audio track of video files. \"\"\"\n",
    "    print(\"Extracting Audio features (MFCCs)...\")\n",
    "    audio_features = []\n",
    "    temp_audio_dir = \"temp_audio\"\n",
    "    if not os.path.exists(temp_audio_dir):\n",
    "        os.makedirs(temp_audio_dir)\n",
    "\n",
    "    num_features = AUDIO_N_MFCC * 2 # Mean and Std Dev for each MFCC\n",
    "\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        # start_time = time.time() # Optional timer\n",
    "        temp_audio_path = os.path.join(temp_audio_dir, f\"temp_{os.path.basename(video_path)}.wav\") # More robust temp name\n",
    "        feature_vector = np.zeros(num_features) # Default to zeros\n",
    "\n",
    "        try:\n",
    "            # Extract audio using moviepy\n",
    "            with VideoFileClip(video_path) as video_clip:\n",
    "                if video_clip.audio is None:\n",
    "                    print(f\"Warning: Video {i} ({os.path.basename(video_path)}) has no audio track. Using zeros.\")\n",
    "                else:\n",
    "                    # Specify logger=None to reduce console output\n",
    "                    video_clip.audio.write_audiofile(temp_audio_path, codec='pcm_s16le', logger=None)\n",
    "\n",
    "                    # Load audio and extract MFCCs using librosa\n",
    "                    # Load with native sample rate, librosa handles resampling if needed by feature extraction\n",
    "                    y, sr = librosa.load(temp_audio_path, sr=None)\n",
    "\n",
    "                    if len(y) > 0: # Check if audio signal is not empty\n",
    "                        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=AUDIO_N_MFCC)\n",
    "                        mfccs_mean = np.mean(mfccs, axis=1)\n",
    "                        mfccs_std = np.std(mfccs, axis=1)\n",
    "                        feature_vector = np.concatenate((mfccs_mean, mfccs_std))\n",
    "                    else:\n",
    "                        print(f\"Warning: Audio signal empty after extraction for video {i}. Using zeros.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting audio features for video {i} ({os.path.basename(video_path)}): {e}. Using zeros.\")\n",
    "            # Ensure feature_vector remains zeros\n",
    "\n",
    "        finally:\n",
    "            # Clean up temporary audio file\n",
    "            if os.path.exists(temp_audio_path):\n",
    "                try:\n",
    "                    os.remove(temp_audio_path)\n",
    "                except Exception as e_rem: # Different variable name for exception\n",
    "                    print(f\"Warning: Could not remove temp audio file {temp_audio_path}: {e_rem}\")\n",
    "\n",
    "        audio_features.append(feature_vector)\n",
    "        # Optional progress print:\n",
    "        # print(f\"Processed audio for video {i+1}/{len(video_paths)} in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # Clean up temp directory if empty\n",
    "    try:\n",
    "        if os.path.exists(temp_audio_dir) and not os.listdir(temp_audio_dir):\n",
    "             os.rmdir(temp_audio_dir)\n",
    "    except Exception as e_rem_dir:\n",
    "        print(f\"Warning: Could not remove temp audio directory {temp_audio_dir}: {e_rem_dir}\")\n",
    "\n",
    "    print(\"Audio feature extraction complete.\")\n",
    "    return np.array(audio_features) # (num_trials, num_audio_features)\n",
    "\n",
    "\n",
    "# 3.3 Visual Feature Extraction (ResNet) - Using sampling rate from version 1 [cite: 491]\n",
    "def extract_visual_features(video_paths):\n",
    "    \"\"\" Extracts aggregated visual features using a pre-trained ResNet model. \"\"\"\n",
    "    print(\"Extracting Visual features (ResNet)...\")\n",
    "\n",
    "    # Load pre-trained ResNet model without the final classification layer\n",
    "    vis_model = models.resnet18(pretrained=True)\n",
    "    vis_model = nn.Sequential(*list(vis_model.children())[:-1]) # Remove the fully connected layer\n",
    "    vis_model = vis_model.to(DEVICE)\n",
    "    vis_model.eval()\n",
    "\n",
    "    # Define image transformations appropriate for ResNet\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    visual_features = []\n",
    "    num_visual_features = 512 # ResNet18 output size before FC layer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, video_path in enumerate(video_paths):\n",
    "            # start_time = time.time() # Optional timer\n",
    "            video_feature_vector = np.zeros(num_visual_features) # Default to zeros\n",
    "\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                if not cap.isOpened():\n",
    "                    print(f\"Warning: Could not open video {i} ({os.path.basename(video_path)}). Using zeros.\")\n",
    "                    visual_features.append(video_feature_vector)\n",
    "                    continue\n",
    "\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if frame_count <= 0:\n",
    "                    print(f\"Warning: Video {i} ({os.path.basename(video_path)}) has no frames or invalid frame count. Using zeros.\")\n",
    "                    visual_features.append(video_feature_vector)\n",
    "                    cap.release()\n",
    "                    continue\n",
    "\n",
    "                # Sample frames evenly across the video - using VISUAL_FRAMES_TO_SAMPLE = 70 [cite: 491]\n",
    "                num_frames_to_sample = min(VISUAL_FRAMES_TO_SAMPLE, frame_count) # Don't sample more than available\n",
    "                if num_frames_to_sample > 0:\n",
    "                    frame_indices = np.linspace(0, frame_count - 1, num_frames_to_sample, dtype=int)\n",
    "                else:\n",
    "                    frame_indices = [] # Should not happen due to frame_count check, but safeguard\n",
    "\n",
    "                frames_data = []\n",
    "                for frame_index in frame_indices:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret:\n",
    "                        # Convert frame BGR -> RGB -> PIL Image -> Apply transforms\n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        img_pil = Image.fromarray(frame_rgb)\n",
    "                        img_tensor = preprocess(img_pil).unsqueeze(0).to(DEVICE) # Add batch dimension\n",
    "                        frames_data.append(img_tensor)\n",
    "                    # else: # Optional: Warn if specific frame fails\n",
    "                        # print(f\"Warning: Could not read frame {frame_index} from video {i}.\")\n",
    "\n",
    "                cap.release()\n",
    "\n",
    "                if frames_data:\n",
    "                    # Stack frame tensors and pass through the model\n",
    "                    batch_tensor = torch.cat(frames_data, dim=0)\n",
    "                    frame_outputs = vis_model(batch_tensor) # (num_sampled_frames, num_visual_features, 1, 1)\n",
    "                    frame_outputs = frame_outputs.squeeze() # Remove trailing 1s -> (num_sampled_frames, num_visual_features)\n",
    "\n",
    "                    # Handle case where only one frame was sampled (squeeze might remove batch dim)\n",
    "                    if frame_outputs.ndim == 1:\n",
    "                        video_feature_vector = frame_outputs.cpu().numpy()\n",
    "                    elif frame_outputs.ndim == 2:\n",
    "                         # Aggregate features (e.g., mean pooling)\n",
    "                         video_feature_vector = torch.mean(frame_outputs, dim=0).cpu().numpy()\n",
    "                    else: # Should not happen\n",
    "                         print(f\"Warning: Unexpected visual feature dimension {frame_outputs.ndim} for video {i}. Using zeros.\")\n",
    "                         video_feature_vector = np.zeros(num_visual_features) # Fallback\n",
    "\n",
    "                else:\n",
    "                    print(f\"Warning: No frames could be processed for video {i} ({os.path.basename(video_path)}). Using zeros.\")\n",
    "                    # video_feature_vector remains zeros\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting visual features for video {i} ({os.path.basename(video_path)}): {e}. Using zeros.\")\n",
    "                if 'cap' in locals() and cap.isOpened():\n",
    "                    cap.release()\n",
    "                # video_feature_vector remains zeros\n",
    "\n",
    "            visual_features.append(video_feature_vector)\n",
    "            # Optional progress print:\n",
    "            # print(f\"Processed visual for video {i+1}/{len(video_paths)} in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    print(\"Visual feature extraction complete.\")\n",
    "    return np.array(visual_features) # (num_trials, num_visual_features)\n",
    "\n",
    "\n",
    "# --- 4. Data Preparation for LOSO ---\n",
    "# Unchanged from pre-process-2 [cite: 27, 515]\n",
    "def prepare_loso(data, subject_mapping):\n",
    "    \"\"\"\n",
    "    Prepares data for LOSO cross-validation using the mandatory subject mapping.\n",
    "    Returns filtered annotations, raw transcriptions, mapped subject IDs, and valid indices.\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for LOSO...\")\n",
    "    annotations = [item['annotation'] for item in data]\n",
    "    transcriptions = [item['transcription'] for item in data] # Keep raw text\n",
    "    video_ids = [item['video_id'] for item in data]\n",
    "\n",
    "    # Map video_ids (trial_ids) to subject IDs using the facial recognition mapping\n",
    "    mapped_subject_ids = []\n",
    "    valid_indices = [] # Keep track of trials with successful subject mapping\n",
    "\n",
    "    for idx, video_id in enumerate(video_ids):\n",
    "        subject_id = subject_mapping.get(video_id)\n",
    "        if subject_id is None:\n",
    "            print(f\"Critical Warning: No subject mapping found for video_id {video_id}. This trial will be skipped in prepare_loso.\")\n",
    "        else:\n",
    "            # Ensure subject IDs are suitable for LOSO grouping (e.g., string or int)\n",
    "            # The current mapping creates strings like 'subject_1' or 'unknown_no_face_1' which is fine.\n",
    "            mapped_subject_ids.append(subject_id)\n",
    "            valid_indices.append(idx)\n",
    "\n",
    "    if not valid_indices:\n",
    "         print(\"Error: No trials have valid subject mappings. Cannot proceed.\")\n",
    "         return np.array([]), [], [], []\n",
    "\n",
    "\n",
    "    if len(valid_indices) < len(data):\n",
    "        print(f\"Warning: {len(data) - len(valid_indices)} trials were skipped due to missing subject mapping.\")\n",
    "\n",
    "    # Filter data based on valid indices\n",
    "    annotations_filtered = np.array(annotations)[valid_indices]\n",
    "    transcriptions_filtered = [transcriptions[i] for i in valid_indices]\n",
    "    # The mapped_subject_ids list already corresponds to the valid_indices\n",
    "    # No, we need to filter the subject_ids based on the final mapping result. Let's rebuild it.\n",
    "    mapped_subject_ids_filtered = [subject_mapping.get(video_ids[i]) for i in valid_indices]\n",
    "\n",
    "\n",
    "    print(f\"Data preparation complete. {len(annotations_filtered)} trials ready for LOSO.\")\n",
    "    # Note: Audio/Visual features are extracted separately AFTER prepare_loso filters trials\n",
    "    return annotations_filtered, transcriptions_filtered, mapped_subject_ids_filtered, valid_indices\n",
    "\n",
    "\n",
    "# --- 5. Multimodal Model Implementation ---\n",
    "# Reverted to the simpler model from pre-process (version 1) [cite: 519, 520]\n",
    "class MultimodalDeceptionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simpler multimodal model for deception detection using NLP, Audio, and Visual features\n",
    "    (Based on the architecture from the first provided script).\n",
    "    \"\"\"\n",
    "    def __init__(self, nlp_input_size, audio_input_size, visual_input_size, hidden_size, num_classes):\n",
    "        super(MultimodalDeceptionModel, self).__init__()\n",
    "        # Simple linear processors for each modality\n",
    "        self.nlp_processor = nn.Linear(nlp_input_size, hidden_size)\n",
    "        self.audio_processor = nn.Linear(audio_input_size, hidden_size)\n",
    "        self.visual_processor = nn.Linear(visual_input_size, hidden_size)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fusion and Classifier layers\n",
    "        self.fusion_dropout = nn.Dropout(0.5) # Dropout after fusion\n",
    "        # Input size to classifier is concatenation of processed features\n",
    "        self.classifier = nn.Linear(hidden_size * 3, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, nlp_data, audio_data, visual_data):\n",
    "        \"\"\" Forward pass processing and fusing features. \"\"\"\n",
    "        # Process each modality through its linear layer and activation\n",
    "        nlp_processed = self.relu(self.nlp_processor(nlp_data))\n",
    "        audio_processed = self.relu(self.audio_processor(audio_data))\n",
    "        visual_processed = self.relu(self.visual_processor(visual_data))\n",
    "\n",
    "        # Fusion by concatenation\n",
    "        fused_features = torch.cat((nlp_processed, audio_processed, visual_processed), dim=1)\n",
    "\n",
    "        # Apply dropout\n",
    "        fused_features = self.fusion_dropout(fused_features)\n",
    "\n",
    "        # Final classification layer\n",
    "        output = self.classifier(fused_features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# --- 6. Training and Evaluation ---\n",
    "# Unchanged from pre-process-2 [cite: 31, 521]\n",
    "def train_evaluate(model,\n",
    "                   nlp_train, audio_train, visual_train, labels_train,\n",
    "                   nlp_test, audio_test, visual_test, labels_test,\n",
    "                   optimizer, criterion, device, epoch):\n",
    "    \"\"\" Trains and evaluates the multimodal model for one epoch. \"\"\"\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Move training data to device\n",
    "    nlp_train_tensor = torch.FloatTensor(nlp_train).to(device)\n",
    "    audio_train_tensor = torch.FloatTensor(audio_train).to(device)\n",
    "    visual_train_tensor = torch.FloatTensor(visual_train).to(device)\n",
    "    labels_train_tensor = torch.LongTensor(labels_train).to(device)\n",
    "\n",
    "    # Forward pass (Training)\n",
    "    outputs = model(nlp_train_tensor, audio_train_tensor, visual_train_tensor)\n",
    "    loss = criterion(outputs, labels_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_item = loss.item() # Get loss value for reporting\n",
    "\n",
    "    # --- Evaluation Phase ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move test data to device\n",
    "        nlp_test_tensor = torch.FloatTensor(nlp_test).to(device)\n",
    "        audio_test_tensor = torch.FloatTensor(audio_test).to(device)\n",
    "        visual_test_tensor = torch.FloatTensor(visual_test).to(device)\n",
    "        labels_test_tensor = torch.LongTensor(labels_test).to(device)\n",
    "\n",
    "        # Forward pass (Evaluation)\n",
    "        outputs_test = model(nlp_test_tensor, audio_test_tensor, visual_test_tensor)\n",
    "        _, predicted = torch.max(outputs_test.data, 1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        labels_test_cpu = labels_test_tensor.cpu().numpy()\n",
    "        predicted_cpu = predicted.cpu().numpy()\n",
    "\n",
    "        accuracy = accuracy_score(labels_test_cpu, predicted_cpu)\n",
    "        f1 = f1_score(labels_test_cpu, predicted_cpu, average='weighted', zero_division=0) # Added zero_division\n",
    "\n",
    "    return accuracy, f1, train_loss_item # Return loss from the training pass\n",
    "\n",
    "\n",
    "# --- 7. Run LOSO Cross-Validation ---\n",
    "# Checkpoint loading logic updated slightly to match the simpler model structure.\n",
    "def run_loso(annotations, nlp_features, audio_features, visual_features, subject_ids,\n",
    "             checkpoint_dir=\"checkpoints\", num_epochs=50, learning_rate=0.001, hidden_size=128):\n",
    "    \"\"\" Runs LOSO cross-validation for the multimodal model. \"\"\"\n",
    "    print(\"Starting LOSO Cross-Validation...\")\n",
    "    loso = LeaveOneGroupOut()\n",
    "    all_accuracies = []\n",
    "    all_f1s = []\n",
    "    all_fold_losses = [] # Store average loss of last epoch per fold\n",
    "\n",
    "    num_seeds = 3 # Number of random seeds for stability [cite: 34, 524]\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    num_classes = len(np.unique(annotations))\n",
    "    if num_classes < 2:\n",
    "        print(f\"Error: Only {num_classes} unique class found. Cannot perform classification.\")\n",
    "        return\n",
    "\n",
    "    # Ensure all feature arrays have the same number of samples as annotations/subject_ids\n",
    "    n_samples = len(annotations)\n",
    "    if not (len(nlp_features) == n_samples and len(audio_features) == n_samples and len(visual_features) == n_samples and len(subject_ids) == n_samples):\n",
    "         print(\"Error: Feature array lengths do not match annotation/subject ID lengths after filtering!\")\n",
    "         print(f\"Lengths: Annotations={n_samples}, NLP={len(nlp_features)}, Audio={len(audio_features)}, Visual={len(visual_features)}, Subjects={len(subject_ids)}\")\n",
    "         return # Cannot proceed if lengths mismatch\n",
    "\n",
    "\n",
    "    fold_num = 0\n",
    "    n_splits = loso.get_n_splits(groups=subject_ids)\n",
    "\n",
    "    for train_index, test_index in loso.split(X=nlp_features, y=annotations, groups=subject_ids): # Provide X and y\n",
    "        fold_num += 1\n",
    "        fold_accuracies_seeds = []\n",
    "        fold_f1s_seeds = []\n",
    "        fold_last_epoch_losses_seeds = [] # Store loss of the last epoch for each seed\n",
    "\n",
    "        # Identify the subject(s) being left out in this fold\n",
    "        test_subjects = np.unique(np.array(subject_ids)[test_index])\n",
    "        test_subject_str = ', '.join(map(str, test_subjects)) # Handle multiple subjects if group has >1 type\n",
    "        print(f\"\\n--- Fold {fold_num}/{n_splits}: Testing on Subject(s) {test_subject_str} ---\")\n",
    "\n",
    "        # Split data for this fold\n",
    "        nlp_train, nlp_test = nlp_features[train_index], nlp_features[test_index]\n",
    "        audio_train, audio_test = audio_features[train_index], audio_features[test_index]\n",
    "        visual_train, visual_test = visual_features[train_index], visual_features[test_index]\n",
    "        labels_train, labels_test = annotations[train_index], annotations[test_index]\n",
    "\n",
    "        # Check if train or test set is empty for this fold\n",
    "        if len(labels_train) == 0 or len(labels_test) == 0:\n",
    "            print(f\"Warning: Skipping Fold {fold_num} due to empty train ({len(labels_train)}) or test ({len(labels_test)}) set.\")\n",
    "            continue\n",
    "\n",
    "        # Get feature dimensions dynamically\n",
    "        try:\n",
    "            nlp_dim = nlp_train.shape[1]\n",
    "            audio_dim = audio_train.shape[1]\n",
    "            visual_dim = visual_train.shape[1]\n",
    "        except IndexError:\n",
    "             print(f\"Error: Could not get feature dimensions in Fold {fold_num}. Skipping fold.\")\n",
    "             print(f\"Shapes: NLP={nlp_train.shape}, Audio={audio_train.shape}, Visual={visual_train.shape}\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        for seed in range(num_seeds):\n",
    "            print(f\"  Seed {seed + 1}/{num_seeds}\")\n",
    "            # Set seeds for reproducibility\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                 torch.cuda.manual_seed_all(seed) # for multi-GPU\n",
    "\n",
    "\n",
    "            # Model initialization, optimizer, and loss function\n",
    "            # Using the simpler model architecture [cite: 519, 520]\n",
    "            model = MultimodalDeceptionModel(\n",
    "                nlp_input_size=nlp_dim,\n",
    "                audio_input_size=audio_dim,\n",
    "                visual_input_size=visual_dim,\n",
    "                hidden_size=hidden_size,\n",
    "                num_classes=num_classes\n",
    "            ).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "            # Define checkpoint file name\n",
    "            checkpoint_file = os.path.join(checkpoint_dir, f\"fold_{fold_num}_seed_{seed + 1}.pth\")\n",
    "\n",
    "            # Check if checkpoint exists and load it\n",
    "            start_epoch = 0\n",
    "            if os.path.exists(checkpoint_file):\n",
    "                try:\n",
    "                    # Load checkpoint onto the correct device\n",
    "                    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    start_epoch = checkpoint['epoch']\n",
    "                    last_loss = checkpoint.get('loss', 'N/A') # Get last saved loss if available\n",
    "                    print(f\"    Resuming training from checkpoint {checkpoint_file} at epoch {start_epoch + 1} (Last saved loss: {last_loss})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Could not load checkpoint {checkpoint_file}: {e}. Starting from scratch.\")\n",
    "                    start_epoch = 0 # Reset start epoch if loading failed\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            best_f1_seed = -1.0 # Track best F1 for saving the best model state for this seed\n",
    "            last_epoch_loss_this_seed = float('nan') # Initialize loss for this seed\n",
    "\n",
    "            for epoch in range(start_epoch, num_epochs):\n",
    "                accuracy, f1, loss = train_evaluate(\n",
    "                    model,\n",
    "                    nlp_train, audio_train, visual_train, labels_train,\n",
    "                    nlp_test, audio_test, visual_test, labels_test,\n",
    "                    optimizer, criterion, DEVICE, epoch)\n",
    "\n",
    "                last_epoch_loss_this_seed = loss # Store loss for the current epoch\n",
    "\n",
    "                # Optional: Save checkpoint based on best F1 for this seed\n",
    "                if f1 > best_f1_seed:\n",
    "                    best_f1_seed = f1\n",
    "                    # print(f\"      Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f}, Acc: {accuracy:.4f}, F1: {f1:.4f} (New best F1, saving checkpoint)\") # Verbose saving log\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1, # Save the epoch number *after* completion\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'f1': f1, # Save F1 score in checkpoint\n",
    "                        'accuracy': accuracy,\n",
    "                        # Save model hyperparams used for this checkpoint\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'nlp_input_size': nlp_dim,\n",
    "                        'audio_input_size': audio_dim,\n",
    "                        'visual_input_size': visual_dim,\n",
    "                        'num_classes': num_classes\n",
    "                    }, checkpoint_file)\n",
    "                # else: # Optional print for epochs that don't improve F1\n",
    "                    # print(f\"      Epoch {epoch + 1}/{num_epochs} - Loss: {loss:.4f}, Acc: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "            # --- After training loop for the seed ---\n",
    "            # Load the best model state saved for this seed based on F1 and evaluate\n",
    "            final_seed_accuracy = float('nan')\n",
    "            final_seed_f1 = float('nan')\n",
    "\n",
    "            if os.path.exists(checkpoint_file):\n",
    "                 try:\n",
    "                    # print(f\"    Loading best checkpoint for seed {seed + 1} to get final metrics...\")\n",
    "                    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "                    # Re-initialize model to ensure clean load\n",
    "                    model = MultimodalDeceptionModel(\n",
    "                            nlp_input_size=checkpoint['nlp_input_size'], # Use saved dims\n",
    "                            audio_input_size=checkpoint['audio_input_size'],\n",
    "                            visual_input_size=checkpoint['visual_input_size'],\n",
    "                            hidden_size=checkpoint['hidden_size'],\n",
    "                            num_classes=checkpoint['num_classes']\n",
    "                            ).to(DEVICE)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    model.eval() # Set model to evaluation mode\n",
    "                    with torch.no_grad():\n",
    "                         nlp_test_tensor = torch.FloatTensor(nlp_test).to(DEVICE)\n",
    "                         audio_test_tensor = torch.FloatTensor(audio_test).to(DEVICE)\n",
    "                         visual_test_tensor = torch.FloatTensor(visual_test).to(DEVICE)\n",
    "                         labels_test_tensor = torch.LongTensor(labels_test).to(DEVICE)\n",
    "                         outputs_final_test = model(nlp_test_tensor, audio_test_tensor, visual_test_tensor)\n",
    "                         _, predicted_final = torch.max(outputs_final_test.data, 1)\n",
    "                         labels_test_cpu = labels_test_tensor.cpu().numpy()\n",
    "                         predicted_final_cpu = predicted_final.cpu().numpy()\n",
    "                         final_seed_accuracy = accuracy_score(labels_test_cpu, predicted_final_cpu)\n",
    "                         final_seed_f1 = f1_score(labels_test_cpu, predicted_final_cpu, average='weighted', zero_division=0)\n",
    "                    print(f\"    Seed {seed + 1} Final Results (Best F1 Model) - Accuracy: {final_seed_accuracy:.4f}, F1: {final_seed_f1:.4f}\")\n",
    "\n",
    "                 except Exception as e_load_final:\n",
    "                      print(f\"    Warning: Could not load or evaluate best checkpoint {checkpoint_file} for final metrics: {e_load_final}\")\n",
    "                      # Use metrics from the very last epoch as fallback\n",
    "                      final_seed_accuracy = accuracy\n",
    "                      final_seed_f1 = f1\n",
    "                      print(f\"    Using results from last epoch instead: Accuracy: {final_seed_accuracy:.4f}, F1: {final_seed_f1:.4f}\")\n",
    "\n",
    "            else:\n",
    "                 print(f\"    Warning: No checkpoint found for seed {seed + 1} after training. Cannot report final metrics.\")\n",
    "\n",
    "\n",
    "            # Store results (from the best model if loaded, else last epoch) for this seed\n",
    "            fold_accuracies_seeds.append(final_seed_accuracy)\n",
    "            fold_f1s_seeds.append(final_seed_f1)\n",
    "            fold_last_epoch_losses_seeds.append(last_epoch_loss_this_seed) # Still store last epoch loss\n",
    "\n",
    "        # --- After all seeds for the fold ---\n",
    "        # Average metrics across seeds for this fold (handle potential NaNs if a seed failed)\n",
    "        if fold_accuracies_seeds: # Check if any seeds ran successfully\n",
    "            avg_fold_accuracy = np.nanmean(fold_accuracies_seeds) # Use nanmean\n",
    "            avg_fold_f1 = np.nanmean(fold_f1s_seeds)\n",
    "            avg_fold_last_epoch_loss = np.nanmean(fold_last_epoch_losses_seeds)\n",
    "\n",
    "            all_accuracies.append(avg_fold_accuracy)\n",
    "            all_f1s.append(avg_fold_f1)\n",
    "            all_fold_losses.append(avg_fold_last_epoch_loss)\n",
    "            print(f\"  Fold {fold_num} Average (across seeds) - Accuracy: {avg_fold_accuracy:.4f}, F1: {avg_fold_f1:.4f}, Last Epoch Loss: {avg_fold_last_epoch_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Fold {fold_num} - No successful seed runs.\")\n",
    "\n",
    "    # --- After all folds ---\n",
    "    # Overall results across folds (handle potential NaNs if a fold failed)\n",
    "    if all_accuracies:\n",
    "        overall_avg_accuracy = np.nanmean(all_accuracies) # Use nanmean\n",
    "        overall_avg_f1 = np.nanmean(all_f1s)\n",
    "        overall_avg_loss = np.nanmean(all_fold_losses)\n",
    "        print(f\"\\n--- Overall LOSO Results (Avg. Across Folds & Seeds) ---\")\n",
    "        print(f\"Overall Average Accuracy: {overall_avg_accuracy:.4f}\")\n",
    "        print(f\"Overall Average F1-score: {overall_avg_f1:.4f}\")\n",
    "        print(f\"Overall Average Last Epoch Loss: {overall_avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n--- No folds completed successfully. Cannot calculate overall results. ---\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 8. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    start_main_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # !!! ADJUST THESE PATHS TO YOUR DATASET LOCATION !!!\n",
    "    # Using paths from the provided scripts [cite: 46, 534]\n",
    "    data_dir = 'Real-life_Deception_Detection_2016' # Example path, adjust as needed\n",
    "    annotation_file = os.path.join(data_dir, 'Annotation', 'All_Gestures_Deceptive and Truthful.csv') # Construct path robustly\n",
    "    checkpoint_dir = \"multimodal_checkpoints_simple\" # Use a different dir for the simple model checkpoints\n",
    "    num_epochs_main = 50 # Adjust number of epochs if needed [cite: 46, 534]\n",
    "    learning_rate_main = 0.001 # [cite: 46, 534]\n",
    "    hidden_size_main = 128 # Hidden dimension for feature processing/fusion [cite: 46, 534]\n",
    "\n",
    "    print(f\"--- Starting Deception Detection Pipeline ---\")\n",
    "    print(f\"Dataset Directory: {os.path.abspath(data_dir)}\")\n",
    "    print(f\"Annotation File: {os.path.abspath(annotation_file)}\")\n",
    "    print(f\"Checkpoints Directory: {os.path.abspath(checkpoint_dir)}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Number of Epochs: {num_epochs_main}\")\n",
    "    print(f\"Learning Rate: {learning_rate_main}\")\n",
    "    print(f\"Hidden Size: {hidden_size_main}\")\n",
    "    print(f\"Visual Frames to Sample: {VISUAL_FRAMES_TO_SAMPLE}\")\n",
    "    print(f\"Audio MFCCs: {AUDIO_N_MFCC}\")\n",
    "    print(f\"-------------------------------------------\")\n",
    "\n",
    "\n",
    "    # --- Workflow ---\n",
    "    # 1. Load Data (Paths, Annotations, Transcriptions)\n",
    "    data = load_data(data_dir, annotation_file)\n",
    "    if not data:\n",
    "        print(\"No data loaded. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Identify Subjects (Mandatory Facial Recognition) [cite: 535]\n",
    "    subject_mapping = identify_subjects_facial_recognition(data)\n",
    "    # print(\"Subject Mapping (Facial Recognition):\", subject_mapping) # Optional print for debugging\n",
    "\n",
    "    # 3. Prepare Data for LOSO (Get filtered annotations, raw transcriptions, mapped IDs) [cite: 535]\n",
    "    annotations, transcriptions_raw, mapped_subject_ids, valid_indices = prepare_loso(data, subject_mapping)\n",
    "    if len(annotations) == 0:\n",
    "        print(\"No valid trials remaining after preparing for LOSO. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Filter original data list based on valid indices from prepare_loso\n",
    "    # This ensures feature extraction only happens for trials included in LOSO\n",
    "    valid_data = [data[i] for i in valid_indices]\n",
    "    video_paths_valid = [item['video_path'] for item in valid_data]\n",
    "\n",
    "    # Check if video_paths_valid is empty, which means no valid trials remained\n",
    "    if not video_paths_valid:\n",
    "         print(\"Error: No valid video paths found after LOSO preparation. Exiting.\")\n",
    "         exit()\n",
    "\n",
    "\n",
    "    # 4. Extract Features for the valid trials [cite: 536]\n",
    "    nlp_features = extract_nlp_features(transcriptions_raw) # Takes raw text list\n",
    "    audio_features = extract_audio_features(video_paths_valid)\n",
    "    visual_features = extract_visual_features(video_paths_valid)\n",
    "\n",
    "    # Sanity check feature shapes before running LOSO\n",
    "    print(f\"Feature shapes after extraction: NLP={nlp_features.shape}, Audio={audio_features.shape}, Visual={visual_features.shape}\")\n",
    "    if not (nlp_features.shape[0] == audio_features.shape[0] == visual_features.shape[0] == len(annotations)):\n",
    "        print(\"Error: Feature array lengths do not match number of annotations after filtering and extraction. Exiting.\")\n",
    "        print(f\"Lengths: Annotations={len(annotations)}, NLP={nlp_features.shape[0]}, Audio={audio_features.shape[0]}, Visual={visual_features.shape[0]}\")\n",
    "        exit()\n",
    "    # Additional check: Ensure features were actually extracted\n",
    "    if nlp_features.size == 0 or audio_features.size == 0 or visual_features.size == 0:\n",
    "        print(\"Error: One or more feature sets are empty after extraction. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # 5. Run LOSO Cross-Validation with Multimodal Features [cite: 537]\n",
    "    run_loso(annotations, nlp_features, audio_features, visual_features, mapped_subject_ids,\n",
    "             checkpoint_dir=checkpoint_dir,\n",
    "             num_epochs=num_epochs_main,\n",
    "             learning_rate=learning_rate_main,\n",
    "             hidden_size=hidden_size_main)\n",
    "\n",
    "    end_main_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {(end_main_time - start_main_time) / 60:.2f} minutes\")\n",
    "    print(\"Multimodal training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
